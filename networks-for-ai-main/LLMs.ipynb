{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca7e30c3-5c31-4305-b12d-ad0180c41e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edcff6c9-7684-4e96-bdb2-d292fcd3e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_MB_size(output):\n",
    "    num_elements = output.numel()\n",
    "    element_size = output.element_size()  # Returns the size in bytes of each element\n",
    "    total_memory_MB = num_elements * element_size / 1024**2\n",
    "    return f\"Size in MB: {total_memory_MB}\"\n",
    "\n",
    "def print_model_intermediate_sizes(model_name, size=512):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=TOKEN)\n",
    "    \n",
    "    # This will hold tuples of layer info and outputs for later sorting and printing\n",
    "    layer_outputs = []\n",
    "\n",
    "    def hook(module, input, output, prefix=\"\"):\n",
    "        layer_num = len(layer_outputs) + 1  # Sequential layer number\n",
    "        layer_info = f\"Layer {layer_num}: {module.__class__.__name__}\"\n",
    "\n",
    "        if output is None:\n",
    "            layer_outputs.append((layer_num, f\"{layer_info} Output is None\"))\n",
    "            return\n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            for i, o in enumerate(output):\n",
    "                if hasattr(o, 'size'):\n",
    "                    size_info = f\"{layer_info} Output {i}: {o.size()} {print_MB_size(o)}\"\n",
    "                    layer_outputs.append((layer_num, size_info))\n",
    "                else:\n",
    "                    layer_outputs.append((layer_num, f\"{layer_info} Output {i}: Output does not have a 'size' attribute\"))\n",
    "        else:\n",
    "            if hasattr(output, 'size'):\n",
    "                size_info = f\"{layer_info} Output: {output.size()} {print_MB_size(output)}\"\n",
    "                layer_outputs.append((layer_num, size_info))\n",
    "            else:\n",
    "                layer_outputs.append((layer_num, f\"{layer_info} Output does not have a 'size' attribute\"))\n",
    "\n",
    "    for layer in model.modules():\n",
    "        if hasattr(layer, 'forward'):\n",
    "            layer.register_forward_hook(hook)\n",
    "\n",
    "    input_ids = torch.randint(0, 20000, (1, size))\n",
    "    attention_mask = torch.ones(1, size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Sort by layer number and print\n",
    "    layer_outputs.sort(key=lambda x: x[0])\n",
    "    for _, output in layer_outputs:\n",
    "        print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ceaacb-3ffd-49e0-ad35-68f451b3f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_memory_MB(tensor):\n",
    "    num_elements = tensor.numel()\n",
    "    element_size = tensor.element_size()  # Returns the size in bytes of each element\n",
    "    total_memory_MB = num_elements * element_size / 1024**2\n",
    "    return total_memory_MB\n",
    "\n",
    "def print_model_weights_size(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=TOKEN)\n",
    "    \n",
    "    total_size_MB = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_size_MB = calculate_memory_MB(param)\n",
    "        total_size_MB += layer_size_MB\n",
    "        print(f\"{name} size: {layer_size_MB:.2f} MB\")\n",
    "    total_size_GB = total_size_MB / 1024\n",
    "    print(f\"Total model size: {total_size_GB:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "761e6798-fb14-4878-b4f8-7bbdd581b4f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae4274f04224ddf9a7453028475502b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 2: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 3: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 4: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 5: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 6: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 6: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 8: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 9: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 9: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 9: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 12: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 13: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 14: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 15: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 16: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 17: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 18: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 18: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 20: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 21: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 22: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 23: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 24: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 24: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 26: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 27: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 27: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 27: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 30: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 31: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 32: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 33: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 34: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 35: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 36: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 36: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 38: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 39: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 40: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 41: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 42: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 42: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 44: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 45: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 45: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 45: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 48: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 49: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 50: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 51: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 52: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 53: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 54: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 54: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 56: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 57: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 58: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 59: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 60: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 60: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 62: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 63: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 63: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 63: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 66: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 67: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 68: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 69: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 70: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 71: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 72: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 72: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 74: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 75: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 76: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 77: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 78: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 78: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 80: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 81: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 81: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 81: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 84: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 85: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 86: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 87: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 88: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 89: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 90: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 90: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 92: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 93: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 94: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 95: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 96: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 96: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 98: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 99: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 99: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 99: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 102: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 103: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 104: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 105: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 106: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 107: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 108: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 108: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 110: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 111: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 112: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 113: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 114: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 114: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 116: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 117: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 117: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 117: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 120: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 121: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 122: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 123: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 124: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 125: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 126: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 126: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 128: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 129: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 130: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 131: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 132: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 132: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 134: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 135: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 135: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 135: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 138: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 139: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 140: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 141: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 142: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 143: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 144: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 144: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 146: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 147: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 148: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 149: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 150: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 150: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 152: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 153: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 153: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 153: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 156: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 157: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 158: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 159: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 160: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 161: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 162: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 162: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 164: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 165: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 166: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 167: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 168: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 168: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 170: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 171: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 171: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 171: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 174: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 175: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 176: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 177: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 178: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 179: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 180: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 180: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 182: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 183: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 184: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 185: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 186: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 186: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 188: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 189: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 189: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 189: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 192: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 193: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 194: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 195: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 196: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 197: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 198: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 198: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 200: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 201: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 202: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 203: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 204: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 204: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 206: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 207: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 207: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 207: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 210: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 211: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 212: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 213: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 214: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 215: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 216: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 216: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 218: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 219: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 220: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 221: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 222: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 222: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 224: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 225: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 225: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 225: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 228: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 229: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 230: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 231: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 232: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 233: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 234: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 234: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 236: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 237: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 238: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 239: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 240: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 240: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 242: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 243: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 243: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 243: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 246: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 247: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 248: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 249: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 250: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 251: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 252: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 252: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 254: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 255: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 256: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 257: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 258: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 258: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 260: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 261: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 261: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 261: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 264: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 265: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 266: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 267: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 268: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 269: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 270: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 270: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 272: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 273: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 274: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 275: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 276: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 276: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 278: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 279: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 279: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 279: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 282: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 283: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 284: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 285: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 286: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 287: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 288: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 288: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 290: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 291: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 292: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 293: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 294: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 294: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 296: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 297: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 297: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 297: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 300: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 301: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 302: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 303: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 304: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 305: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 306: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 306: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 308: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 309: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 310: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 311: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 312: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 312: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 314: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 315: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 315: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 315: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 318: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 319: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 320: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 321: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 322: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 323: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 324: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 324: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 326: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 327: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 328: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 329: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 330: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 330: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 332: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 333: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 333: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 333: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 336: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 337: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 338: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 339: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 340: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 341: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 342: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 342: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 344: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 345: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 346: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 347: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 348: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 348: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 350: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 351: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 351: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 351: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 354: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 355: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 356: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 357: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 358: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 359: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 360: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 360: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 362: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 363: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 364: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 365: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 366: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 366: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 368: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 369: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 369: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 369: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 372: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 373: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 374: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 375: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 376: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 377: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 378: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 378: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 380: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 381: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 382: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 383: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 384: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 384: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 386: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 387: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 387: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 387: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 390: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 391: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 392: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 393: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 394: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 395: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 396: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 396: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 398: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 399: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 400: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 401: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 402: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 402: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 404: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 405: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 405: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 405: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 408: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 409: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 410: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 411: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 412: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 413: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 414: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 414: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 416: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 417: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 418: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 419: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 420: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 420: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 422: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 423: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 423: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 423: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 426: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 427: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 428: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 429: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 430: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 431: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 432: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 432: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 434: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 435: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 436: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 437: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 438: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 438: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 440: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 441: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 441: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 441: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 444: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 445: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 446: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 447: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 448: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 449: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 450: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 450: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 452: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 453: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 454: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 455: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 456: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 456: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 458: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 459: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 459: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 459: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 462: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 463: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 464: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 465: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 466: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 467: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 468: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 468: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 470: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 471: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 472: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 473: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 474: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 474: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 476: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 477: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 477: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 477: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 480: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 481: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 482: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 483: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 484: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 485: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 486: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 486: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 488: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 489: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 490: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 491: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 492: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 492: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 494: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 495: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 495: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 495: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 498: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 499: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 500: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 501: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 502: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 503: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 504: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 504: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 506: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 507: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 508: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 509: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 510: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 510: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 512: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 513: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 513: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 513: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 516: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 517: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 518: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 519: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 520: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 521: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 522: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 522: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 524: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 525: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 526: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 527: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 528: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 528: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 530: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 531: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 531: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 531: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 534: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 535: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 536: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 537: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 538: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 539: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 540: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 540: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 542: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 543: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 544: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 545: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 546: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 546: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 548: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 549: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 549: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 549: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 552: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 553: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 554: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 555: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 556: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 557: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 558: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 558: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 560: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 561: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 562: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 563: Linear Output: torch.Size([1, 8000, 1024]) Size in MB: 31.25\n",
      "Layer 564: MistralRotaryEmbedding Output 0: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 564: MistralRotaryEmbedding Output 1: torch.Size([8000, 128]) Size in MB: 3.90625\n",
      "Layer 566: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 567: MistralSdpaAttention Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 567: MistralSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 567: MistralSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 570: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 571: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 572: SiLU Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 573: Linear Output: torch.Size([1, 8000, 14336]) Size in MB: 437.5\n",
      "Layer 574: Linear Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 575: MistralMLP Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 576: MistralDecoderLayer Output 0: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 576: MistralDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 578: MistralRMSNorm Output: torch.Size([1, 8000, 4096]) Size in MB: 125.0\n",
      "Layer 579: MistralModel Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "# Example usage for Mistral 7B \n",
    "print_model_intermediate_sizes('mistralai/Mistral-7B-v0.1', size=8000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a405add-a050-407d-83a7-92f1ac98224c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719f0fbf41674bf79d5623c6af950e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd41b937bc19445ca5ffcb92477831bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc3bf11e7fa4cbf8bbebba38d69448e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fac3682bef340b492644bb761412075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight size: 500.00 MB\n",
      "layers.0.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.0.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.0.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.0.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.0.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.0.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.0.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.0.input_layernorm.weight size: 0.02 MB\n",
      "layers.0.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.1.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.1.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.1.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.1.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.1.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.1.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.1.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.1.input_layernorm.weight size: 0.02 MB\n",
      "layers.1.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.2.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.2.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.2.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.2.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.2.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.2.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.2.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.2.input_layernorm.weight size: 0.02 MB\n",
      "layers.2.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.3.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.3.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.3.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.3.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.3.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.3.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.3.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.3.input_layernorm.weight size: 0.02 MB\n",
      "layers.3.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.4.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.4.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.4.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.4.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.4.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.4.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.4.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.4.input_layernorm.weight size: 0.02 MB\n",
      "layers.4.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.5.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.5.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.5.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.5.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.5.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.5.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.5.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.5.input_layernorm.weight size: 0.02 MB\n",
      "layers.5.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.6.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.6.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.6.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.6.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.6.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.6.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.6.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.6.input_layernorm.weight size: 0.02 MB\n",
      "layers.6.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.7.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.7.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.7.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.7.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.7.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.7.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.7.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.7.input_layernorm.weight size: 0.02 MB\n",
      "layers.7.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.8.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.8.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.8.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.8.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.8.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.8.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.8.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.8.input_layernorm.weight size: 0.02 MB\n",
      "layers.8.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.9.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.9.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.9.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.9.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.9.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.9.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.9.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.9.input_layernorm.weight size: 0.02 MB\n",
      "layers.9.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.10.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.10.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.10.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.10.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.10.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.10.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.10.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.10.input_layernorm.weight size: 0.02 MB\n",
      "layers.10.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.11.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.11.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.11.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.11.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.11.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.11.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.11.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.11.input_layernorm.weight size: 0.02 MB\n",
      "layers.11.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.12.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.12.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.12.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.12.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.12.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.12.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.12.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.12.input_layernorm.weight size: 0.02 MB\n",
      "layers.12.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.13.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.13.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.13.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.13.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.13.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.13.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.13.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.13.input_layernorm.weight size: 0.02 MB\n",
      "layers.13.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.14.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.14.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.14.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.14.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.14.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.14.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.14.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.14.input_layernorm.weight size: 0.02 MB\n",
      "layers.14.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.15.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.15.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.15.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.15.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.15.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.15.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.15.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.15.input_layernorm.weight size: 0.02 MB\n",
      "layers.15.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.16.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.16.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.16.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.16.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.16.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.16.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.16.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.16.input_layernorm.weight size: 0.02 MB\n",
      "layers.16.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.17.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.17.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.17.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.17.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.17.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.17.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.17.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.17.input_layernorm.weight size: 0.02 MB\n",
      "layers.17.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.18.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.18.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.18.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.18.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.18.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.18.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.18.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.18.input_layernorm.weight size: 0.02 MB\n",
      "layers.18.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.19.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.19.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.19.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.19.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.19.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.19.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.19.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.19.input_layernorm.weight size: 0.02 MB\n",
      "layers.19.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.20.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.20.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.20.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.20.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.20.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.20.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.20.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.20.input_layernorm.weight size: 0.02 MB\n",
      "layers.20.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.21.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.21.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.21.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.21.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.21.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.21.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.21.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.21.input_layernorm.weight size: 0.02 MB\n",
      "layers.21.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.22.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.22.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.22.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.22.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.22.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.22.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.22.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.22.input_layernorm.weight size: 0.02 MB\n",
      "layers.22.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.23.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.23.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.23.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.23.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.23.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.23.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.23.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.23.input_layernorm.weight size: 0.02 MB\n",
      "layers.23.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.24.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.24.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.24.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.24.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.24.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.24.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.24.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.24.input_layernorm.weight size: 0.02 MB\n",
      "layers.24.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.25.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.25.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.25.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.25.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.25.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.25.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.25.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.25.input_layernorm.weight size: 0.02 MB\n",
      "layers.25.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.26.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.26.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.26.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.26.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.26.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.26.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.26.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.26.input_layernorm.weight size: 0.02 MB\n",
      "layers.26.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.27.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.27.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.27.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.27.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.27.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.27.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.27.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.27.input_layernorm.weight size: 0.02 MB\n",
      "layers.27.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.28.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.28.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.28.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.28.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.28.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.28.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.28.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.28.input_layernorm.weight size: 0.02 MB\n",
      "layers.28.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.29.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.29.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.29.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.29.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.29.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.29.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.29.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.29.input_layernorm.weight size: 0.02 MB\n",
      "layers.29.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.30.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.30.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.30.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.30.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.30.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.30.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.30.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.30.input_layernorm.weight size: 0.02 MB\n",
      "layers.30.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.31.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.31.self_attn.k_proj.weight size: 16.00 MB\n",
      "layers.31.self_attn.v_proj.weight size: 16.00 MB\n",
      "layers.31.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.31.mlp.gate_proj.weight size: 224.00 MB\n",
      "layers.31.mlp.up_proj.weight size: 224.00 MB\n",
      "layers.31.mlp.down_proj.weight size: 224.00 MB\n",
      "layers.31.input_layernorm.weight size: 0.02 MB\n",
      "layers.31.post_attention_layernorm.weight size: 0.02 MB\n",
      "norm.weight size: 0.02 MB\n",
      "Total model size: 26.49 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_weights_size('mistralai/Mistral-7B-v0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfb4a01c-92d0-479d-8dfd-bdfac1cb13e8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cda7a8209043f6a03f02856af822b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badc040301324095a40df29d441fc451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fa1380063f442d86729eaf00ec129c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899b3f9de5854fbcb3b4a176c82275bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b39d0b25d4a44e490d0f132750e36ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9f26c8beb04671be2243cac1197291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 2: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 3: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 4: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 5: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 6: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 6: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 8: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 9: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 9: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 9: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 12: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 13: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 14: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 15: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 16: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 17: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 18: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 18: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 20: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 21: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 22: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 23: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 24: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 24: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 26: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 27: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 27: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 27: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 30: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 31: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 32: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 33: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 34: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 35: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 36: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 36: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 38: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 39: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 40: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 41: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 42: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 42: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 44: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 45: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 45: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 45: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 48: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 49: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 50: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 51: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 52: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 53: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 54: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 54: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 56: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 57: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 58: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 59: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 60: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 60: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 62: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 63: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 63: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 63: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 66: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 67: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 68: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 69: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 70: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 71: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 72: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 72: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 74: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 75: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 76: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 77: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 78: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 78: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 80: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 81: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 81: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 81: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 84: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 85: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 86: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 87: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 88: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 89: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 90: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 90: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 92: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 93: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 94: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 95: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 96: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 96: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 98: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 99: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 99: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 99: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 102: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 103: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 104: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 105: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 106: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 107: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 108: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 108: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 110: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 111: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 112: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 113: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 114: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 114: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 116: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 117: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 117: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 117: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 120: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 121: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 122: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 123: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 124: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 125: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 126: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 126: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 128: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 129: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 130: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 131: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 132: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 132: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 134: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 135: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 135: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 135: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 138: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 139: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 140: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 141: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 142: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 143: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 144: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 144: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 146: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 147: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 148: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 149: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 150: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 150: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 152: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 153: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 153: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 153: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 156: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 157: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 158: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 159: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 160: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 161: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 162: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 162: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 164: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 165: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 166: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 167: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 168: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 168: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 170: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 171: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 171: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 171: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 174: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 175: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 176: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 177: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 178: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 179: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 180: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 180: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 182: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 183: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 184: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 185: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 186: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 186: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 188: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 189: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 189: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 189: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 192: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 193: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 194: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 195: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 196: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 197: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 198: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 198: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 200: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 201: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 202: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 203: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 204: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 204: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 206: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 207: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 207: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 207: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 210: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 211: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 212: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 213: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 214: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 215: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 216: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 216: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 218: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 219: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 220: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 221: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 222: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 222: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 224: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 225: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 225: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 225: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 228: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 229: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 230: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 231: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 232: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 233: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 234: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 234: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 236: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 237: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 238: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 239: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 240: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 240: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 242: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 243: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 243: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 243: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 246: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 247: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 248: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 249: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 250: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 251: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 252: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 252: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 254: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 255: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 256: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 257: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 258: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 258: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 260: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 261: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 261: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 261: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 264: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 265: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 266: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 267: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 268: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 269: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 270: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 270: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 272: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 273: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 274: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 275: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 276: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 276: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 278: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 279: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 279: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 279: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 282: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 283: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 284: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 285: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 286: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 287: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 288: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 288: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 290: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 291: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 292: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 293: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 294: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 294: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 296: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 297: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 297: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 297: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 300: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 301: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 302: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 303: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 304: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 305: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 306: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 306: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 308: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 309: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 310: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 311: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 312: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 312: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 314: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 315: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 315: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 315: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 318: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 319: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 320: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 321: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 322: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 323: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 324: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 324: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 326: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 327: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 328: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 329: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 330: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 330: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 332: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 333: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 333: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 333: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 336: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 337: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 338: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 339: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 340: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 341: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 342: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 342: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 344: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 345: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 346: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 347: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 348: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 348: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 350: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 351: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 351: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 351: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 354: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 355: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 356: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 357: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 358: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 359: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 360: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 360: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 362: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 363: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 364: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 365: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 366: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 366: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 368: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 369: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 369: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 369: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 372: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 373: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 374: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 375: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 376: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 377: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 378: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 378: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 380: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 381: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 382: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 383: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 384: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 384: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 386: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 387: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 387: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 387: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 390: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 391: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 392: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 393: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 394: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 395: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 396: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 396: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 398: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 399: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 400: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 401: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 402: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 402: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 404: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 405: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 405: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 405: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 408: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 409: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 410: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 411: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 412: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 413: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 414: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 414: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 416: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 417: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 418: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 419: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 420: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 420: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 422: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 423: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 423: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 423: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 426: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 427: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 428: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 429: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 430: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 431: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 432: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 432: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 434: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 435: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 436: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 437: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 438: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 438: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 440: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 441: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 441: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 441: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 444: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 445: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 446: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 447: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 448: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 449: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 450: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 450: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 452: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 453: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 454: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 455: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 456: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 456: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 458: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 459: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 459: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 459: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 462: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 463: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 464: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 465: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 466: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 467: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 468: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 468: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 470: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 471: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 472: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 473: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 474: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 474: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 476: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 477: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 477: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 477: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 480: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 481: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 482: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 483: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 484: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 485: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 486: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 486: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 488: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 489: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 490: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 491: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 492: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 492: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 494: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 495: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 495: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 495: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 498: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 499: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 500: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 501: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 502: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 503: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 504: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 504: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 506: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 507: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 508: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 509: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 510: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 510: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 512: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 513: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 513: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 513: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 516: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 517: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 518: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 519: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 520: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 521: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 522: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 522: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 524: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 525: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 526: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 527: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 528: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 528: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 530: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 531: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 531: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 531: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 534: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 535: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 536: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 537: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 538: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 539: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 540: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 540: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 542: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 543: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 544: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 545: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 546: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 546: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 548: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 549: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 549: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 549: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 552: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 553: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 554: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 555: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 556: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 557: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 558: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 558: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 560: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 561: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 562: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 563: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 564: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 564: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 566: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 567: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 567: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 567: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 570: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 571: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 572: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 573: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 574: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 575: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 576: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 576: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 578: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 579: LlamaModel Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "print_model_intermediate_sizes('meta-llama/Llama-2-7b-hf', size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00cb880b-2859-4494-8c4a-9856d2849818",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e431c12b17ce400bb1c5ea97ef23507b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight size: 500.00 MB\n",
      "layers.0.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.0.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.0.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.0.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.0.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.0.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.0.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.0.input_layernorm.weight size: 0.02 MB\n",
      "layers.0.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.1.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.1.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.1.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.1.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.1.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.1.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.1.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.1.input_layernorm.weight size: 0.02 MB\n",
      "layers.1.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.2.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.2.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.2.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.2.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.2.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.2.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.2.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.2.input_layernorm.weight size: 0.02 MB\n",
      "layers.2.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.3.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.3.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.3.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.3.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.3.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.3.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.3.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.3.input_layernorm.weight size: 0.02 MB\n",
      "layers.3.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.4.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.4.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.4.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.4.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.4.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.4.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.4.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.4.input_layernorm.weight size: 0.02 MB\n",
      "layers.4.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.5.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.5.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.5.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.5.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.5.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.5.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.5.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.5.input_layernorm.weight size: 0.02 MB\n",
      "layers.5.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.6.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.6.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.6.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.6.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.6.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.6.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.6.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.6.input_layernorm.weight size: 0.02 MB\n",
      "layers.6.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.7.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.7.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.7.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.7.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.7.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.7.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.7.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.7.input_layernorm.weight size: 0.02 MB\n",
      "layers.7.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.8.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.8.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.8.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.8.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.8.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.8.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.8.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.8.input_layernorm.weight size: 0.02 MB\n",
      "layers.8.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.9.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.9.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.9.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.9.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.9.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.9.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.9.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.9.input_layernorm.weight size: 0.02 MB\n",
      "layers.9.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.10.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.10.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.10.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.10.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.10.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.10.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.10.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.10.input_layernorm.weight size: 0.02 MB\n",
      "layers.10.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.11.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.11.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.11.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.11.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.11.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.11.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.11.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.11.input_layernorm.weight size: 0.02 MB\n",
      "layers.11.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.12.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.12.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.12.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.12.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.12.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.12.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.12.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.12.input_layernorm.weight size: 0.02 MB\n",
      "layers.12.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.13.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.13.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.13.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.13.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.13.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.13.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.13.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.13.input_layernorm.weight size: 0.02 MB\n",
      "layers.13.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.14.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.14.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.14.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.14.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.14.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.14.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.14.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.14.input_layernorm.weight size: 0.02 MB\n",
      "layers.14.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.15.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.15.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.15.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.15.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.15.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.15.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.15.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.15.input_layernorm.weight size: 0.02 MB\n",
      "layers.15.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.16.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.16.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.16.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.16.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.16.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.16.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.16.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.16.input_layernorm.weight size: 0.02 MB\n",
      "layers.16.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.17.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.17.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.17.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.17.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.17.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.17.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.17.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.17.input_layernorm.weight size: 0.02 MB\n",
      "layers.17.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.18.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.18.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.18.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.18.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.18.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.18.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.18.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.18.input_layernorm.weight size: 0.02 MB\n",
      "layers.18.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.19.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.19.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.19.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.19.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.19.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.19.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.19.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.19.input_layernorm.weight size: 0.02 MB\n",
      "layers.19.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.20.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.20.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.20.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.20.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.20.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.20.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.20.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.20.input_layernorm.weight size: 0.02 MB\n",
      "layers.20.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.21.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.21.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.21.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.21.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.21.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.21.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.21.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.21.input_layernorm.weight size: 0.02 MB\n",
      "layers.21.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.22.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.22.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.22.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.22.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.22.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.22.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.22.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.22.input_layernorm.weight size: 0.02 MB\n",
      "layers.22.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.23.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.23.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.23.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.23.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.23.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.23.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.23.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.23.input_layernorm.weight size: 0.02 MB\n",
      "layers.23.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.24.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.24.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.24.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.24.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.24.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.24.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.24.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.24.input_layernorm.weight size: 0.02 MB\n",
      "layers.24.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.25.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.25.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.25.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.25.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.25.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.25.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.25.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.25.input_layernorm.weight size: 0.02 MB\n",
      "layers.25.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.26.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.26.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.26.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.26.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.26.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.26.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.26.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.26.input_layernorm.weight size: 0.02 MB\n",
      "layers.26.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.27.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.27.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.27.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.27.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.27.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.27.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.27.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.27.input_layernorm.weight size: 0.02 MB\n",
      "layers.27.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.28.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.28.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.28.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.28.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.28.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.28.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.28.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.28.input_layernorm.weight size: 0.02 MB\n",
      "layers.28.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.29.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.29.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.29.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.29.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.29.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.29.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.29.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.29.input_layernorm.weight size: 0.02 MB\n",
      "layers.29.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.30.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.30.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.30.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.30.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.30.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.30.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.30.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.30.input_layernorm.weight size: 0.02 MB\n",
      "layers.30.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.31.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.31.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.31.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.31.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.31.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.31.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.31.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.31.input_layernorm.weight size: 0.02 MB\n",
      "layers.31.post_attention_layernorm.weight size: 0.02 MB\n",
      "norm.weight size: 0.02 MB\n",
      "Total model size: 24.61 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_weights_size('meta-llama/Llama-2-7b-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9dcbecc-4148-4138-abbe-1716f54e3b97",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e050ea9e98954dac81b117374cb4e3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06b4339a0e34ba7b588daac9e569e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205bc675d3354d62ace1d53692e1621b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce32d2be947418591200978116cfdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f334e5ded5ab40d0b14cbb5dc0a31ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539fd20ae8104fbe84726feb5ae2ffce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424af2642c62483aa777b984f041acfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b347755746247809de0eccbe90a2aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 2: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 3: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 4: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 5: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 6: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 6: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 8: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 9: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 9: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 9: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 12: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 13: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 14: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 15: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 16: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 17: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 18: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 18: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 20: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 21: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 22: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 23: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 24: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 24: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 26: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 27: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 27: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 27: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 30: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 31: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 32: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 33: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 34: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 35: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 36: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 36: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 38: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 39: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 40: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 41: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 42: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 42: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 44: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 45: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 45: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 45: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 48: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 49: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 50: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 51: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 52: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 53: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 54: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 54: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 56: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 57: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 58: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 59: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 60: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 60: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 62: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 63: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 63: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 63: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 66: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 67: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 68: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 69: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 70: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 71: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 72: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 72: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 74: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 75: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 76: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 77: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 78: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 78: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 80: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 81: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 81: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 81: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 84: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 85: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 86: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 87: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 88: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 89: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 90: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 90: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 92: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 93: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 94: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 95: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 96: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 96: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 98: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 99: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 99: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 99: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 102: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 103: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 104: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 105: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 106: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 107: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 108: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 108: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 110: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 111: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 112: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 113: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 114: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 114: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 116: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 117: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 117: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 117: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 120: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 121: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 122: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 123: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 124: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 125: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 126: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 126: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 128: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 129: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 130: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 131: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 132: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 132: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 134: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 135: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 135: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 135: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 138: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 139: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 140: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 141: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 142: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 143: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 144: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 144: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 146: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 147: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 148: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 149: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 150: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 150: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 152: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 153: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 153: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 153: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 156: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 157: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 158: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 159: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 160: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 161: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 162: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 162: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 164: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 165: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 166: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 167: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 168: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 168: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 170: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 171: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 171: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 171: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 174: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 175: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 176: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 177: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 178: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 179: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 180: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 180: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 182: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 183: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 184: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 185: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 186: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 186: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 188: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 189: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 189: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 189: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 192: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 193: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 194: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 195: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 196: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 197: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 198: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 198: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 200: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 201: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 202: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 203: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 204: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 204: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 206: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 207: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 207: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 207: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 210: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 211: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 212: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 213: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 214: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 215: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 216: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 216: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 218: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 219: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 220: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 221: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 222: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 222: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 224: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 225: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 225: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 225: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 228: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 229: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 230: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 231: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 232: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 233: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 234: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 234: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 236: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 237: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 238: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 239: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 240: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 240: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 242: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 243: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 243: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 243: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 246: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 247: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 248: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 249: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 250: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 251: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 252: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 252: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 254: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 255: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 256: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 257: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 258: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 258: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 260: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 261: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 261: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 261: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 264: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 265: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 266: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 267: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 268: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 269: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 270: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 270: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 272: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 273: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 274: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 275: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 276: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 276: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 278: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 279: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 279: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 279: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 282: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 283: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 284: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 285: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 286: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 287: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 288: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 288: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 290: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 291: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 292: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 293: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 294: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 294: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 296: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 297: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 297: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 297: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 300: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 301: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 302: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 303: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 304: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 305: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 306: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 306: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 308: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 309: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 310: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 311: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 312: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 312: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 314: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 315: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 315: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 315: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 318: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 319: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 320: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 321: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 322: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 323: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 324: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 324: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 326: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 327: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 328: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 329: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 330: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 330: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 332: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 333: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 333: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 333: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 336: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 337: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 338: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 339: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 340: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 341: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 342: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 342: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 344: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 345: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 346: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 347: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 348: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 348: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 350: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 351: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 351: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 351: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 354: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 355: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 356: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 357: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 358: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 359: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 360: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 360: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 362: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 363: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 364: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 365: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 366: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 366: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 368: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 369: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 369: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 369: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 372: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 373: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 374: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 375: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 376: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 377: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 378: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 378: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 380: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 381: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 382: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 383: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 384: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 384: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 386: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 387: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 387: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 387: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 390: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 391: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 392: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 393: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 394: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 395: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 396: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 396: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 398: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 399: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 400: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 401: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 402: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 402: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 404: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 405: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 405: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 405: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 408: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 409: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 410: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 411: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 412: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 413: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 414: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 414: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 416: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 417: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 418: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 419: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 420: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 420: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 422: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 423: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 423: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 423: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 426: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 427: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 428: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 429: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 430: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 431: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 432: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 432: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 434: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 435: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 436: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 437: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 438: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 438: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 440: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 441: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 441: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 441: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 444: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 445: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 446: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 447: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 448: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 449: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 450: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 450: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 452: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 453: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 454: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 455: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 456: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 456: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 458: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 459: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 459: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 459: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 462: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 463: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 464: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 465: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 466: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 467: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 468: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 468: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 470: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 471: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 472: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 473: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 474: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 474: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 476: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 477: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 477: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 477: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 480: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 481: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 482: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 483: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 484: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 485: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 486: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 486: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 488: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 489: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 490: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 491: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 492: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 492: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 494: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 495: GemmaSdpaAttention Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 495: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 495: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 498: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 499: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 500: GELUActivation Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 501: Linear Output: torch.Size([1, 512, 24576]) Size in MB: 48.0\n",
      "Layer 502: Linear Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 503: GemmaMLP Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 504: GemmaDecoderLayer Output 0: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 504: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 506: GemmaRMSNorm Output: torch.Size([1, 512, 3072]) Size in MB: 6.0\n",
      "Layer 507: GemmaModel Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "print_model_intermediate_sizes('google/gemma-7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff15785b-58c4-4c20-a7aa-95b7e1a24a21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684f6d153e944fa9b6c7e7bc8bc444ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight size: 3000.00 MB\n",
      "layers.0.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.0.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.0.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.0.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.0.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.0.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.0.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.0.input_layernorm.weight size: 0.01 MB\n",
      "layers.0.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.1.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.1.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.1.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.1.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.1.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.1.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.1.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.1.input_layernorm.weight size: 0.01 MB\n",
      "layers.1.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.2.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.2.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.2.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.2.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.2.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.2.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.2.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.2.input_layernorm.weight size: 0.01 MB\n",
      "layers.2.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.3.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.3.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.3.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.3.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.3.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.3.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.3.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.3.input_layernorm.weight size: 0.01 MB\n",
      "layers.3.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.4.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.4.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.4.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.4.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.4.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.4.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.4.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.4.input_layernorm.weight size: 0.01 MB\n",
      "layers.4.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.5.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.5.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.5.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.5.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.5.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.5.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.5.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.5.input_layernorm.weight size: 0.01 MB\n",
      "layers.5.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.6.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.6.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.6.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.6.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.6.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.6.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.6.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.6.input_layernorm.weight size: 0.01 MB\n",
      "layers.6.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.7.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.7.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.7.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.7.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.7.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.7.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.7.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.7.input_layernorm.weight size: 0.01 MB\n",
      "layers.7.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.8.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.8.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.8.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.8.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.8.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.8.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.8.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.8.input_layernorm.weight size: 0.01 MB\n",
      "layers.8.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.9.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.9.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.9.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.9.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.9.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.9.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.9.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.9.input_layernorm.weight size: 0.01 MB\n",
      "layers.9.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.10.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.10.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.10.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.10.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.10.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.10.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.10.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.10.input_layernorm.weight size: 0.01 MB\n",
      "layers.10.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.11.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.11.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.11.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.11.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.11.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.11.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.11.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.11.input_layernorm.weight size: 0.01 MB\n",
      "layers.11.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.12.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.12.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.12.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.12.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.12.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.12.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.12.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.12.input_layernorm.weight size: 0.01 MB\n",
      "layers.12.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.13.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.13.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.13.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.13.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.13.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.13.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.13.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.13.input_layernorm.weight size: 0.01 MB\n",
      "layers.13.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.14.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.14.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.14.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.14.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.14.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.14.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.14.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.14.input_layernorm.weight size: 0.01 MB\n",
      "layers.14.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.15.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.15.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.15.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.15.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.15.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.15.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.15.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.15.input_layernorm.weight size: 0.01 MB\n",
      "layers.15.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.16.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.16.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.16.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.16.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.16.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.16.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.16.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.16.input_layernorm.weight size: 0.01 MB\n",
      "layers.16.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.17.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.17.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.17.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.17.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.17.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.17.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.17.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.17.input_layernorm.weight size: 0.01 MB\n",
      "layers.17.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.18.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.18.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.18.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.18.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.18.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.18.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.18.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.18.input_layernorm.weight size: 0.01 MB\n",
      "layers.18.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.19.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.19.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.19.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.19.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.19.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.19.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.19.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.19.input_layernorm.weight size: 0.01 MB\n",
      "layers.19.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.20.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.20.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.20.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.20.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.20.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.20.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.20.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.20.input_layernorm.weight size: 0.01 MB\n",
      "layers.20.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.21.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.21.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.21.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.21.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.21.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.21.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.21.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.21.input_layernorm.weight size: 0.01 MB\n",
      "layers.21.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.22.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.22.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.22.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.22.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.22.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.22.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.22.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.22.input_layernorm.weight size: 0.01 MB\n",
      "layers.22.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.23.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.23.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.23.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.23.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.23.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.23.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.23.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.23.input_layernorm.weight size: 0.01 MB\n",
      "layers.23.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.24.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.24.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.24.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.24.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.24.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.24.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.24.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.24.input_layernorm.weight size: 0.01 MB\n",
      "layers.24.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.25.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.25.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.25.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.25.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.25.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.25.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.25.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.25.input_layernorm.weight size: 0.01 MB\n",
      "layers.25.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.26.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.26.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.26.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.26.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.26.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.26.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.26.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.26.input_layernorm.weight size: 0.01 MB\n",
      "layers.26.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.27.self_attn.q_proj.weight size: 48.00 MB\n",
      "layers.27.self_attn.k_proj.weight size: 48.00 MB\n",
      "layers.27.self_attn.v_proj.weight size: 48.00 MB\n",
      "layers.27.self_attn.o_proj.weight size: 48.00 MB\n",
      "layers.27.mlp.gate_proj.weight size: 288.00 MB\n",
      "layers.27.mlp.up_proj.weight size: 288.00 MB\n",
      "layers.27.mlp.down_proj.weight size: 288.00 MB\n",
      "layers.27.input_layernorm.weight size: 0.01 MB\n",
      "layers.27.post_attention_layernorm.weight size: 0.01 MB\n",
      "norm.weight size: 0.01 MB\n",
      "Total model size: 31.81 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_weights_size('google/gemma-7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38ce629e-e635-4d9a-a209-a2aa71f92a2f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae9d64ff1e046dda839741f956a1853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b40f35291cf43d39f3dad40d386a807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1564d7d0ad74a02bdda52b325b79c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1183c6fc14458bbb5944af594fd1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec98afa3df4472cab61b0a813384131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37054dbd82c6482b9de55699bd7b81af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 2: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 3: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 4: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 5: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 6: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 6: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 8: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 9: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 9: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 9: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 12: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 13: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 14: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 15: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 16: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 17: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 18: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 18: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 20: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 21: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 22: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 23: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 24: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 24: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 26: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 27: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 27: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 27: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 30: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 31: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 32: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 33: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 34: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 35: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 36: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 36: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 38: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 39: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 40: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 41: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 42: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 42: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 44: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 45: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 45: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 45: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 48: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 49: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 50: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 51: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 52: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 53: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 54: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 54: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 56: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 57: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 58: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 59: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 60: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 60: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 62: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 63: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 63: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 63: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 66: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 67: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 68: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 69: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 70: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 71: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 72: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 72: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 74: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 75: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 76: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 77: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 78: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 78: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 80: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 81: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 81: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 81: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 84: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 85: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 86: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 87: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 88: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 89: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 90: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 90: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 92: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 93: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 94: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 95: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 96: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 96: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 98: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 99: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 99: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 99: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 102: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 103: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 104: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 105: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 106: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 107: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 108: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 108: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 110: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 111: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 112: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 113: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 114: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 114: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 116: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 117: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 117: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 117: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 120: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 121: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 122: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 123: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 124: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 125: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 126: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 126: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 128: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 129: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 130: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 131: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 132: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 132: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 134: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 135: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 135: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 135: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 138: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 139: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 140: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 141: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 142: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 143: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 144: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 144: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 146: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 147: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 148: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 149: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 150: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 150: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 152: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 153: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 153: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 153: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 156: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 157: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 158: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 159: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 160: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 161: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 162: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 162: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 164: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 165: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 166: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 167: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 168: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 168: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 170: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 171: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 171: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 171: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 174: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 175: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 176: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 177: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 178: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 179: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 180: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 180: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 182: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 183: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 184: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 185: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 186: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 186: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 188: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 189: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 189: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 189: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 192: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 193: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 194: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 195: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 196: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 197: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 198: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 198: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 200: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 201: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 202: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 203: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 204: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 204: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 206: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 207: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 207: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 207: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 210: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 211: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 212: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 213: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 214: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 215: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 216: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 216: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 218: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 219: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 220: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 221: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 222: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 222: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 224: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 225: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 225: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 225: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 228: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 229: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 230: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 231: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 232: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 233: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 234: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 234: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 236: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 237: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 238: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 239: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 240: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 240: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 242: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 243: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 243: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 243: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 246: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 247: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 248: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 249: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 250: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 251: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 252: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 252: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 254: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 255: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 256: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 257: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 258: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 258: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 260: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 261: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 261: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 261: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 264: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 265: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 266: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 267: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 268: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 269: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 270: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 270: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 272: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 273: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 274: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 275: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 276: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 276: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 278: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 279: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 279: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 279: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 282: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 283: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 284: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 285: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 286: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 287: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 288: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 288: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 290: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 291: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 292: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 293: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 294: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 294: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 296: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 297: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 297: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 297: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 300: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 301: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 302: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 303: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 304: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 305: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 306: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 306: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 308: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 309: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 310: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 311: Linear Output: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 312: GemmaRotaryEmbedding Output 0: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 312: GemmaRotaryEmbedding Output 1: torch.Size([1, 512, 256]) Size in MB: 0.5\n",
      "Layer 314: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 315: GemmaSdpaAttention Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 315: GemmaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 315: GemmaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 318: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 319: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 320: GELUActivation Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 321: Linear Output: torch.Size([1, 512, 16384]) Size in MB: 32.0\n",
      "Layer 322: Linear Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 323: GemmaMLP Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 324: GemmaDecoderLayer Output 0: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 324: GemmaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 326: GemmaRMSNorm Output: torch.Size([1, 512, 2048]) Size in MB: 4.0\n",
      "Layer 327: GemmaModel Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "print_model_intermediate_sizes('google/gemma-2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51139a23-0e1c-45fb-b60b-23a74c84ff42",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c2b776575446f09c29f736c60cd50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight size: 2000.00 MB\n",
      "layers.0.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.0.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.0.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.0.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.0.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.0.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.0.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.0.input_layernorm.weight size: 0.01 MB\n",
      "layers.0.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.1.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.1.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.1.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.1.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.1.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.1.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.1.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.1.input_layernorm.weight size: 0.01 MB\n",
      "layers.1.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.2.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.2.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.2.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.2.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.2.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.2.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.2.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.2.input_layernorm.weight size: 0.01 MB\n",
      "layers.2.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.3.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.3.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.3.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.3.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.3.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.3.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.3.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.3.input_layernorm.weight size: 0.01 MB\n",
      "layers.3.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.4.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.4.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.4.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.4.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.4.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.4.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.4.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.4.input_layernorm.weight size: 0.01 MB\n",
      "layers.4.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.5.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.5.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.5.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.5.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.5.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.5.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.5.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.5.input_layernorm.weight size: 0.01 MB\n",
      "layers.5.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.6.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.6.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.6.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.6.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.6.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.6.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.6.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.6.input_layernorm.weight size: 0.01 MB\n",
      "layers.6.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.7.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.7.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.7.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.7.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.7.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.7.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.7.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.7.input_layernorm.weight size: 0.01 MB\n",
      "layers.7.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.8.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.8.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.8.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.8.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.8.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.8.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.8.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.8.input_layernorm.weight size: 0.01 MB\n",
      "layers.8.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.9.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.9.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.9.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.9.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.9.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.9.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.9.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.9.input_layernorm.weight size: 0.01 MB\n",
      "layers.9.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.10.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.10.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.10.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.10.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.10.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.10.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.10.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.10.input_layernorm.weight size: 0.01 MB\n",
      "layers.10.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.11.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.11.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.11.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.11.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.11.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.11.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.11.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.11.input_layernorm.weight size: 0.01 MB\n",
      "layers.11.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.12.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.12.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.12.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.12.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.12.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.12.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.12.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.12.input_layernorm.weight size: 0.01 MB\n",
      "layers.12.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.13.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.13.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.13.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.13.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.13.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.13.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.13.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.13.input_layernorm.weight size: 0.01 MB\n",
      "layers.13.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.14.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.14.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.14.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.14.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.14.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.14.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.14.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.14.input_layernorm.weight size: 0.01 MB\n",
      "layers.14.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.15.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.15.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.15.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.15.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.15.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.15.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.15.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.15.input_layernorm.weight size: 0.01 MB\n",
      "layers.15.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.16.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.16.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.16.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.16.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.16.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.16.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.16.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.16.input_layernorm.weight size: 0.01 MB\n",
      "layers.16.post_attention_layernorm.weight size: 0.01 MB\n",
      "layers.17.self_attn.q_proj.weight size: 16.00 MB\n",
      "layers.17.self_attn.k_proj.weight size: 2.00 MB\n",
      "layers.17.self_attn.v_proj.weight size: 2.00 MB\n",
      "layers.17.self_attn.o_proj.weight size: 16.00 MB\n",
      "layers.17.mlp.gate_proj.weight size: 128.00 MB\n",
      "layers.17.mlp.up_proj.weight size: 128.00 MB\n",
      "layers.17.mlp.down_proj.weight size: 128.00 MB\n",
      "layers.17.input_layernorm.weight size: 0.01 MB\n",
      "layers.17.post_attention_layernorm.weight size: 0.01 MB\n",
      "norm.weight size: 0.01 MB\n",
      "Total model size: 9.34 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_weights_size('google/gemma-2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "336a0e1e-d4ad-42ee-874a-eb5f30c71a61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9436c90d666b4f08958405697fdc8928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832f27fefc8d409aa065ad980d84feb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44181c95aa443209552f17b68cdaee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ee5c41c3a44c8e9d1372f9fd5b5823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d7abeff1ad479c862d15f55e79b8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4026743e10f641e99f839b34c952d2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 2: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 3: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 4: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 5: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 6: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 6: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 8: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 9: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 9: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 9: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 12: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 13: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 14: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 15: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 16: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 17: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 18: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 18: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 20: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 21: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 22: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 23: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 24: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 24: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 26: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 27: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 27: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 27: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 30: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 31: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 32: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 33: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 34: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 35: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 36: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 36: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 38: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 39: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 40: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 41: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 42: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 42: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 44: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 45: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 45: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 45: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 48: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 49: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 50: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 51: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 52: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 53: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 54: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 54: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 56: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 57: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 58: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 59: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 60: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 60: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 62: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 63: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 63: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 63: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 66: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 67: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 68: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 69: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 70: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 71: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 72: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 72: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 74: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 75: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 76: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 77: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 78: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 78: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 80: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 81: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 81: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 81: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 84: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 85: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 86: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 87: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 88: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 89: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 90: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 90: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 92: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 93: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 94: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 95: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 96: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 96: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 98: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 99: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 99: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 99: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 102: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 103: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 104: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 105: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 106: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 107: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 108: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 108: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 110: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 111: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 112: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 113: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 114: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 114: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 116: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 117: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 117: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 117: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 120: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 121: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 122: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 123: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 124: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 125: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 126: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 126: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 128: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 129: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 130: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 131: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 132: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 132: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 134: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 135: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 135: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 135: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 138: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 139: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 140: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 141: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 142: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 143: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 144: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 144: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 146: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 147: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 148: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 149: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 150: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 150: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 152: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 153: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 153: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 153: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 156: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 157: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 158: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 159: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 160: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 161: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 162: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 162: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 164: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 165: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 166: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 167: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 168: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 168: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 170: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 171: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 171: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 171: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 174: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 175: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 176: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 177: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 178: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 179: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 180: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 180: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 182: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 183: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 184: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 185: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 186: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 186: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 188: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 189: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 189: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 189: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 192: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 193: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 194: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 195: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 196: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 197: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 198: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 198: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 200: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 201: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 202: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 203: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 204: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 204: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 206: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 207: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 207: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 207: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 210: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 211: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 212: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 213: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 214: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 215: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 216: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 216: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 218: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 219: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 220: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 221: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 222: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 222: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 224: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 225: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 225: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 225: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 228: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 229: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 230: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 231: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 232: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 233: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 234: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 234: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 236: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 237: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 238: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 239: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 240: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 240: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 242: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 243: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 243: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 243: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 246: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 247: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 248: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 249: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 250: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 251: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 252: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 252: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 254: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 255: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 256: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 257: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 258: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 258: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 260: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 261: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 261: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 261: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 264: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 265: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 266: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 267: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 268: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 269: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 270: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 270: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 272: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 273: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 274: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 275: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 276: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 276: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 278: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 279: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 279: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 279: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 282: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 283: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 284: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 285: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 286: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 287: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 288: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 288: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 290: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 291: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 292: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 293: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 294: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 294: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 296: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 297: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 297: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 297: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 300: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 301: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 302: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 303: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 304: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 305: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 306: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 306: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 308: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 309: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 310: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 311: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 312: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 312: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 314: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 315: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 315: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 315: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 318: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 319: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 320: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 321: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 322: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 323: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 324: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 324: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 326: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 327: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 328: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 329: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 330: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 330: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 332: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 333: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 333: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 333: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 336: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 337: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 338: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 339: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 340: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 341: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 342: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 342: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 344: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 345: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 346: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 347: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 348: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 348: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 350: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 351: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 351: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 351: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 354: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 355: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 356: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 357: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 358: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 359: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 360: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 360: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 362: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 363: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 364: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 365: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 366: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 366: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 368: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 369: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 369: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 369: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 372: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 373: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 374: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 375: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 376: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 377: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 378: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 378: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 380: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 381: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 382: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 383: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 384: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 384: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 386: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 387: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 387: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 387: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 390: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 391: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 392: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 393: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 394: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 395: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 396: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 396: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 398: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 399: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 400: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 401: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 402: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 402: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 404: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 405: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 405: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 405: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 408: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 409: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 410: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 411: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 412: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 413: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 414: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 414: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 416: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 417: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 418: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 419: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 420: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 420: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 422: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 423: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 423: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 423: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 426: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 427: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 428: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 429: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 430: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 431: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 432: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 432: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 434: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 435: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 436: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 437: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 438: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 438: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 440: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 441: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 441: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 441: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 444: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 445: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 446: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 447: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 448: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 449: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 450: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 450: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 452: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 453: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 454: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 455: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 456: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 456: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 458: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 459: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 459: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 459: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 462: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 463: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 464: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 465: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 466: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 467: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 468: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 468: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 470: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 471: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 472: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 473: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 474: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 474: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 476: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 477: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 477: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 477: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 480: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 481: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 482: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 483: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 484: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 485: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 486: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 486: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 488: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 489: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 490: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 491: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 492: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 492: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 494: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 495: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 495: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 495: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 498: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 499: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 500: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 501: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 502: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 503: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 504: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 504: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 506: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 507: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 508: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 509: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 510: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 510: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 512: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 513: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 513: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 513: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 516: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 517: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 518: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 519: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 520: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 521: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 522: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 522: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 524: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 525: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 526: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 527: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 528: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 528: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 530: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 531: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 531: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 531: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 534: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 535: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 536: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 537: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 538: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 539: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 540: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 540: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 542: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 543: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 544: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 545: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 546: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 546: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 548: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 549: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 549: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 549: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 552: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 553: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 554: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 555: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 556: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 557: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 558: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 558: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 560: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 561: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 562: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 563: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 564: LlamaRotaryEmbedding Output 0: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 564: LlamaRotaryEmbedding Output 1: torch.Size([1, 512, 128]) Size in MB: 0.25\n",
      "Layer 566: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 567: LlamaSdpaAttention Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 567: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 567: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 570: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 571: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 572: SiLU Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 573: Linear Output: torch.Size([1, 512, 11008]) Size in MB: 21.5\n",
      "Layer 574: Linear Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 575: LlamaMLP Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 576: LlamaDecoderLayer Output 0: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 576: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 578: LlamaRMSNorm Output: torch.Size([1, 512, 4096]) Size in MB: 8.0\n",
      "Layer 579: LlamaModel Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "print_model_intermediate_sizes('lmsys/vicuna-7b-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ece101d3-efff-4982-bc25-2452801f5e6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd68b5e81914db298bc2c5ec72aeca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight size: 500.00 MB\n",
      "layers.0.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.0.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.0.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.0.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.0.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.0.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.0.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.0.input_layernorm.weight size: 0.02 MB\n",
      "layers.0.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.1.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.1.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.1.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.1.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.1.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.1.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.1.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.1.input_layernorm.weight size: 0.02 MB\n",
      "layers.1.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.2.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.2.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.2.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.2.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.2.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.2.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.2.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.2.input_layernorm.weight size: 0.02 MB\n",
      "layers.2.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.3.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.3.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.3.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.3.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.3.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.3.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.3.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.3.input_layernorm.weight size: 0.02 MB\n",
      "layers.3.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.4.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.4.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.4.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.4.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.4.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.4.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.4.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.4.input_layernorm.weight size: 0.02 MB\n",
      "layers.4.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.5.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.5.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.5.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.5.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.5.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.5.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.5.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.5.input_layernorm.weight size: 0.02 MB\n",
      "layers.5.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.6.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.6.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.6.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.6.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.6.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.6.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.6.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.6.input_layernorm.weight size: 0.02 MB\n",
      "layers.6.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.7.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.7.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.7.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.7.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.7.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.7.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.7.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.7.input_layernorm.weight size: 0.02 MB\n",
      "layers.7.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.8.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.8.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.8.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.8.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.8.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.8.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.8.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.8.input_layernorm.weight size: 0.02 MB\n",
      "layers.8.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.9.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.9.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.9.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.9.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.9.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.9.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.9.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.9.input_layernorm.weight size: 0.02 MB\n",
      "layers.9.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.10.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.10.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.10.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.10.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.10.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.10.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.10.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.10.input_layernorm.weight size: 0.02 MB\n",
      "layers.10.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.11.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.11.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.11.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.11.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.11.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.11.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.11.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.11.input_layernorm.weight size: 0.02 MB\n",
      "layers.11.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.12.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.12.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.12.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.12.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.12.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.12.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.12.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.12.input_layernorm.weight size: 0.02 MB\n",
      "layers.12.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.13.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.13.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.13.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.13.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.13.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.13.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.13.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.13.input_layernorm.weight size: 0.02 MB\n",
      "layers.13.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.14.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.14.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.14.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.14.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.14.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.14.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.14.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.14.input_layernorm.weight size: 0.02 MB\n",
      "layers.14.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.15.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.15.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.15.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.15.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.15.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.15.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.15.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.15.input_layernorm.weight size: 0.02 MB\n",
      "layers.15.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.16.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.16.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.16.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.16.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.16.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.16.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.16.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.16.input_layernorm.weight size: 0.02 MB\n",
      "layers.16.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.17.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.17.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.17.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.17.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.17.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.17.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.17.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.17.input_layernorm.weight size: 0.02 MB\n",
      "layers.17.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.18.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.18.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.18.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.18.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.18.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.18.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.18.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.18.input_layernorm.weight size: 0.02 MB\n",
      "layers.18.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.19.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.19.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.19.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.19.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.19.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.19.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.19.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.19.input_layernorm.weight size: 0.02 MB\n",
      "layers.19.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.20.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.20.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.20.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.20.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.20.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.20.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.20.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.20.input_layernorm.weight size: 0.02 MB\n",
      "layers.20.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.21.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.21.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.21.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.21.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.21.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.21.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.21.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.21.input_layernorm.weight size: 0.02 MB\n",
      "layers.21.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.22.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.22.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.22.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.22.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.22.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.22.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.22.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.22.input_layernorm.weight size: 0.02 MB\n",
      "layers.22.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.23.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.23.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.23.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.23.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.23.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.23.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.23.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.23.input_layernorm.weight size: 0.02 MB\n",
      "layers.23.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.24.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.24.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.24.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.24.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.24.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.24.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.24.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.24.input_layernorm.weight size: 0.02 MB\n",
      "layers.24.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.25.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.25.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.25.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.25.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.25.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.25.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.25.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.25.input_layernorm.weight size: 0.02 MB\n",
      "layers.25.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.26.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.26.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.26.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.26.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.26.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.26.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.26.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.26.input_layernorm.weight size: 0.02 MB\n",
      "layers.26.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.27.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.27.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.27.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.27.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.27.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.27.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.27.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.27.input_layernorm.weight size: 0.02 MB\n",
      "layers.27.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.28.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.28.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.28.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.28.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.28.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.28.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.28.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.28.input_layernorm.weight size: 0.02 MB\n",
      "layers.28.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.29.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.29.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.29.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.29.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.29.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.29.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.29.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.29.input_layernorm.weight size: 0.02 MB\n",
      "layers.29.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.30.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.30.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.30.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.30.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.30.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.30.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.30.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.30.input_layernorm.weight size: 0.02 MB\n",
      "layers.30.post_attention_layernorm.weight size: 0.02 MB\n",
      "layers.31.self_attn.q_proj.weight size: 64.00 MB\n",
      "layers.31.self_attn.k_proj.weight size: 64.00 MB\n",
      "layers.31.self_attn.v_proj.weight size: 64.00 MB\n",
      "layers.31.self_attn.o_proj.weight size: 64.00 MB\n",
      "layers.31.mlp.gate_proj.weight size: 172.00 MB\n",
      "layers.31.mlp.up_proj.weight size: 172.00 MB\n",
      "layers.31.mlp.down_proj.weight size: 172.00 MB\n",
      "layers.31.input_layernorm.weight size: 0.02 MB\n",
      "layers.31.post_attention_layernorm.weight size: 0.02 MB\n",
      "norm.weight size: 0.02 MB\n",
      "Total model size: 24.61 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_weights_size('lmsys/vicuna-7b-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63d8b2a-a8f6-477a-bcb9-963fa649ed36",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311de7a463cc4b9fb7e0deeb28efe733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e166e307ee3462aac4d3f60f7bf19de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bd16e9cea64846b8d7f35fea00f384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62853aae274e4c48aaec4ed1d3e5af70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39767064ccba4e9b822e540972d16054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9189ebf9a27348e493182d41fe620939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d94f87228749a3be509a8ba5a12296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 2: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 3: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 4: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 5: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 6: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 7: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 7: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 9: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 10: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 10: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 10: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 13: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 14: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 15: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 16: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 17: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 18: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 19: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 19: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 21: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 22: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 23: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 24: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 25: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 25: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 27: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 28: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 28: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 28: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 31: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 32: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 33: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 34: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 35: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 36: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 37: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 37: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 39: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 40: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 41: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 42: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 43: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 43: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 45: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 46: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 46: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 46: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 49: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 50: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 51: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 52: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 53: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 54: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 55: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 55: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 57: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 58: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 59: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 60: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 61: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 61: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 63: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 64: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 64: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 64: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 67: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 68: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 69: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 70: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 71: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 72: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 73: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 73: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 75: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 76: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 77: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 78: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 79: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 79: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 81: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 82: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 82: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 82: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 85: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 86: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 87: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 88: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 89: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 90: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 91: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 91: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 93: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 94: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 95: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 96: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 97: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 97: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 99: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 100: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 100: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 100: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 103: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 104: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 105: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 106: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 107: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 108: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 109: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 109: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 111: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 112: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 113: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 114: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 115: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 115: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 117: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 118: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 118: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 118: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 121: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 122: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 123: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 124: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 125: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 126: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 127: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 127: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 129: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 130: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 131: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 132: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 133: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 133: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 135: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 136: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 136: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 136: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 139: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 140: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 141: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 142: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 143: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 144: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 145: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 145: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 147: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 148: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 149: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 150: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 151: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 151: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 153: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 154: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 154: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 154: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 157: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 158: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 159: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 160: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 161: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 162: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 163: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 163: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 165: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 166: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 167: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 168: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 169: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 169: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 171: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 172: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 172: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 172: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 175: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 176: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 177: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 178: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 179: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 180: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 181: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 181: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 183: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 184: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 185: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 186: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 187: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 187: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 189: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 190: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 190: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 190: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 193: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 194: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 195: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 196: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 197: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 198: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 199: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 199: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 201: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 202: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 203: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 204: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 205: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 205: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 207: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 208: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 208: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 208: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 211: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 212: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 213: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 214: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 215: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 216: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 217: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 217: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 219: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 220: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 221: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 222: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 223: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 223: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 225: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 226: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 226: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 226: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 229: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 230: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 231: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 232: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 233: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 234: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 235: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 235: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 237: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 238: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 239: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 240: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 241: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 241: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 243: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 244: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 244: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 244: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 247: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 248: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 249: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 250: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 251: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 252: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 253: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 253: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 255: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 256: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 257: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 258: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 259: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 259: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 261: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 262: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 262: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 262: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 265: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 266: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 267: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 268: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 269: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 270: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 271: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 271: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 273: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 274: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 275: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 276: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 277: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 277: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 279: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 280: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 280: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 280: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 283: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 284: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 285: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 286: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 287: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 288: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 289: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 289: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 291: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 292: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 293: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 294: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 295: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 295: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 297: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 298: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 298: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 298: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 301: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 302: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 303: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 304: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 305: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 306: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 307: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 307: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 309: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 310: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 311: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 312: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 313: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 313: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 315: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 316: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 316: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 316: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 319: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 320: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 321: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 322: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 323: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 324: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 325: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 325: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 327: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 328: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 329: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 330: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 331: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 331: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 333: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 334: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 334: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 334: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 337: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 338: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 339: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 340: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 341: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 342: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 343: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 343: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 345: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 346: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 347: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 348: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 349: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 349: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 351: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 352: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 352: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 352: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 355: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 356: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 357: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 358: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 359: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 360: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 361: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 361: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 363: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 364: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 365: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 366: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 367: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 367: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 369: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 370: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 370: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 370: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 373: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 374: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 375: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 376: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 377: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 378: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 379: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 379: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 381: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 382: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 383: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 384: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 385: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 385: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 387: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 388: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 388: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 388: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 391: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 392: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 393: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 394: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 395: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 396: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 397: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 397: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 399: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 400: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 401: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 402: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 403: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 403: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 405: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 406: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 406: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 406: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 409: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 410: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 411: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 412: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 413: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 414: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 415: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 415: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 417: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 418: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 419: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 420: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 421: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 421: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 423: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 424: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 424: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 424: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 427: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 428: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 429: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 430: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 431: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 432: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 433: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 433: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 435: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 436: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 437: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 438: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 439: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 439: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 441: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 442: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 442: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 442: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 445: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 446: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 447: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 448: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 449: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 450: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 451: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 451: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 453: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 454: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 455: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 456: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 457: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 457: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 459: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 460: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 460: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 460: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 463: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 464: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 465: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 466: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 467: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 468: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 469: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 469: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 471: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 472: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 473: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 474: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 475: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 475: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 477: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 478: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 478: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 478: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 481: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 482: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 483: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 484: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 485: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 486: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 487: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 487: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 489: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 490: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 491: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 492: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 493: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 493: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 495: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 496: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 496: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 496: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 499: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 500: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 501: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 502: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 503: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 504: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 505: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 505: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 507: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 508: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 509: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 510: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 511: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 511: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 513: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 514: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 514: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 514: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 517: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 518: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 519: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 520: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 521: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 522: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 523: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 523: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 525: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 526: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 527: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 528: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 529: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 529: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 531: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 532: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 532: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 532: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 535: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 536: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 537: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 538: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 539: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 540: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 541: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 541: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 543: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 544: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 545: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 546: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 547: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 547: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 549: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 550: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 550: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 550: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 553: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 554: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 555: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 556: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 557: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 558: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 559: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 559: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 561: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 562: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 563: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 564: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 565: PhiRotaryEmbedding Output 0: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 565: PhiRotaryEmbedding Output 1: torch.Size([512, 32]) Size in MB: 0.0625\n",
      "Layer 567: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 568: PhiAttention Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 568: PhiAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 568: PhiAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 571: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 572: Linear Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 573: NewGELUActivation Output: torch.Size([1, 512, 10240]) Size in MB: 20.0\n",
      "Layer 574: Linear Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 575: PhiMLP Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 576: Dropout Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 577: PhiDecoderLayer Output 0: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 577: PhiDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 579: LayerNorm Output: torch.Size([1, 512, 2560]) Size in MB: 5.0\n",
      "Layer 580: PhiModel Output does not have a 'size' attribute\n",
      "Layer 581: Linear Output: torch.Size([1, 512, 51200]) Size in MB: 100.0\n",
      "Layer 582: PhiForCausalLM Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "print_model_intermediate_sizes('microsoft/phi-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d772246-5c67-4ef0-9903-f389cfb6143d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e966db8898e4815b0152637c98b9d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight size: 500.00 MB\n",
      "model.layers.0.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.0.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.0.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.0.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.0.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.0.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.0.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.0.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.0.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.0.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.0.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.0.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.0.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.0.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.1.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.1.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.1.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.1.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.1.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.1.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.1.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.1.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.1.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.1.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.1.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.1.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.1.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.1.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.2.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.2.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.2.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.2.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.2.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.2.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.2.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.2.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.2.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.2.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.2.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.2.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.2.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.2.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.3.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.3.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.3.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.3.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.3.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.3.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.3.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.3.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.3.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.3.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.3.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.3.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.3.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.3.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.4.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.4.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.4.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.4.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.4.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.4.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.4.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.4.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.4.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.4.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.4.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.4.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.4.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.4.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.5.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.5.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.5.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.5.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.5.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.5.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.5.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.5.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.5.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.5.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.5.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.5.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.5.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.5.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.6.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.6.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.6.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.6.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.6.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.6.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.6.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.6.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.6.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.6.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.6.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.6.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.6.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.6.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.7.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.7.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.7.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.7.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.7.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.7.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.7.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.7.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.7.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.7.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.7.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.7.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.7.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.7.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.8.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.8.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.8.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.8.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.8.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.8.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.8.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.8.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.8.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.8.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.8.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.8.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.8.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.8.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.9.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.9.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.9.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.9.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.9.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.9.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.9.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.9.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.9.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.9.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.9.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.9.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.9.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.9.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.10.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.10.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.10.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.10.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.10.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.10.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.10.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.10.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.10.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.10.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.10.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.10.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.10.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.10.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.11.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.11.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.11.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.11.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.11.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.11.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.11.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.11.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.11.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.11.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.11.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.11.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.11.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.11.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.12.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.12.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.12.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.12.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.12.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.12.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.12.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.12.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.12.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.12.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.12.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.12.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.12.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.12.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.13.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.13.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.13.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.13.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.13.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.13.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.13.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.13.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.13.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.13.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.13.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.13.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.13.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.13.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.14.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.14.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.14.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.14.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.14.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.14.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.14.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.14.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.14.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.14.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.14.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.14.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.14.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.14.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.15.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.15.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.15.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.15.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.15.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.15.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.15.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.15.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.15.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.15.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.15.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.15.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.15.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.15.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.16.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.16.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.16.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.16.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.16.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.16.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.16.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.16.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.16.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.16.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.16.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.16.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.16.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.16.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.17.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.17.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.17.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.17.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.17.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.17.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.17.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.17.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.17.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.17.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.17.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.17.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.17.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.17.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.18.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.18.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.18.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.18.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.18.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.18.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.18.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.18.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.18.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.18.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.18.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.18.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.18.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.18.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.19.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.19.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.19.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.19.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.19.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.19.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.19.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.19.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.19.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.19.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.19.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.19.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.19.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.19.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.20.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.20.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.20.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.20.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.20.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.20.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.20.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.20.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.20.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.20.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.20.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.20.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.20.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.20.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.21.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.21.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.21.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.21.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.21.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.21.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.21.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.21.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.21.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.21.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.21.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.21.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.21.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.21.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.22.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.22.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.22.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.22.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.22.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.22.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.22.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.22.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.22.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.22.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.22.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.22.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.22.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.22.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.23.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.23.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.23.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.23.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.23.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.23.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.23.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.23.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.23.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.23.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.23.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.23.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.23.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.23.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.24.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.24.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.24.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.24.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.24.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.24.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.24.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.24.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.24.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.24.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.24.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.24.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.24.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.24.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.25.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.25.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.25.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.25.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.25.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.25.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.25.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.25.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.25.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.25.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.25.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.25.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.25.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.25.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.26.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.26.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.26.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.26.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.26.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.26.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.26.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.26.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.26.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.26.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.26.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.26.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.26.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.26.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.27.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.27.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.27.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.27.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.27.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.27.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.27.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.27.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.27.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.27.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.27.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.27.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.27.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.27.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.28.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.28.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.28.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.28.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.28.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.28.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.28.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.28.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.28.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.28.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.28.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.28.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.28.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.28.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.29.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.29.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.29.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.29.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.29.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.29.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.29.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.29.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.29.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.29.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.29.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.29.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.29.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.29.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.30.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.30.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.30.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.30.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.30.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.30.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.30.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.30.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.30.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.30.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.30.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.30.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.30.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.30.input_layernorm.bias size: 0.01 MB\n",
      "model.layers.31.self_attn.q_proj.weight size: 25.00 MB\n",
      "model.layers.31.self_attn.q_proj.bias size: 0.01 MB\n",
      "model.layers.31.self_attn.k_proj.weight size: 25.00 MB\n",
      "model.layers.31.self_attn.k_proj.bias size: 0.01 MB\n",
      "model.layers.31.self_attn.v_proj.weight size: 25.00 MB\n",
      "model.layers.31.self_attn.v_proj.bias size: 0.01 MB\n",
      "model.layers.31.self_attn.dense.weight size: 25.00 MB\n",
      "model.layers.31.self_attn.dense.bias size: 0.01 MB\n",
      "model.layers.31.mlp.fc1.weight size: 100.00 MB\n",
      "model.layers.31.mlp.fc1.bias size: 0.04 MB\n",
      "model.layers.31.mlp.fc2.weight size: 100.00 MB\n",
      "model.layers.31.mlp.fc2.bias size: 0.01 MB\n",
      "model.layers.31.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.31.input_layernorm.bias size: 0.01 MB\n",
      "model.final_layernorm.weight size: 0.01 MB\n",
      "model.final_layernorm.bias size: 0.01 MB\n",
      "lm_head.weight size: 500.00 MB\n",
      "lm_head.bias size: 0.20 MB\n",
      "Total model size: 10.36 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_weights_size('microsoft/phi-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789c8619-2b28-43ac-bbd0-b900c4703346",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d730256922044ecad7735c249c6ca53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 2: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 3: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 4: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 5: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 6: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 6: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 8: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 9: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 9: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 9: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 12: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 13: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 14: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 15: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 16: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 17: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 18: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 18: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 20: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 21: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 22: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 23: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 24: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 24: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 26: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 27: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 27: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 27: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 30: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 31: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 32: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 33: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 34: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 35: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 36: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 36: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 38: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 39: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 40: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 41: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 42: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 42: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 44: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 45: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 45: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 45: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 48: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 49: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 50: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 51: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 52: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 53: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 54: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 54: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 56: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 57: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 58: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 59: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 60: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 60: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 62: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 63: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 63: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 63: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 66: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 67: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 68: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 69: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 70: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 71: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 72: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 72: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 74: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 75: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 76: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 77: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 78: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 78: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 80: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 81: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 81: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 81: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 84: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 85: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 86: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 87: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 88: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 89: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 90: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 90: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 92: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 93: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 94: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 95: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 96: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 96: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 98: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 99: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 99: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 99: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 102: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 103: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 104: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 105: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 106: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 107: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 108: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 108: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 110: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 111: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 112: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 113: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 114: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 114: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 116: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 117: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 117: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 117: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 120: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 121: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 122: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 123: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 124: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 125: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 126: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 126: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 128: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 129: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 130: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 131: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 132: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 132: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 134: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 135: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 135: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 135: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 138: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 139: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 140: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 141: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 142: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 143: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 144: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 144: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 146: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 147: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 148: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 149: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 150: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 150: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 152: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 153: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 153: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 153: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 156: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 157: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 158: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 159: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 160: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 161: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 162: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 162: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 164: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 165: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 166: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 167: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 168: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 168: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 170: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 171: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 171: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 171: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 174: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 175: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 176: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 177: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 178: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 179: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 180: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 180: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 182: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 183: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 184: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 185: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 186: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 186: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 188: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 189: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 189: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 189: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 192: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 193: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 194: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 195: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 196: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 197: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 198: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 198: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 200: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 201: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 202: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 203: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 204: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 204: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 206: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 207: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 207: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 207: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 210: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 211: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 212: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 213: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 214: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 215: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 216: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 216: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 218: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 219: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 220: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 221: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 222: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 222: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 224: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 225: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 225: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 225: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 228: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 229: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 230: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 231: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 232: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 233: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 234: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 234: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 236: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 237: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 238: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 239: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 240: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 240: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 242: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 243: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 243: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 243: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 246: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 247: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 248: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 249: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 250: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 251: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 252: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 252: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 254: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 255: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 256: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 257: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 258: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 258: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 260: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 261: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 261: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 261: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 264: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 265: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 266: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 267: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 268: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 269: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 270: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 270: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 272: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 273: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 274: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 275: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 276: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 276: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 278: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 279: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 279: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 279: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 282: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 283: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 284: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 285: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 286: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 287: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 288: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 288: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 290: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 291: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 292: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 293: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 294: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 294: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 296: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 297: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 297: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 297: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 300: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 301: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 302: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 303: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 304: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 305: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 306: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 306: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 308: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 309: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 310: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 311: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 312: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 312: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 314: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 315: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 315: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 315: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 318: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 319: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 320: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 321: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 322: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 323: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 324: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 324: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 326: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 327: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 328: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 329: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 330: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 330: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 332: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 333: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 333: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 333: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 336: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 337: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 338: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 339: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 340: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 341: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 342: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 342: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 344: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 345: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 346: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 347: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 348: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 348: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 350: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 351: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 351: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 351: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 354: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 355: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 356: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 357: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 358: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 359: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 360: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 360: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 362: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 363: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 364: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 365: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 366: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 366: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 368: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 369: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 369: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 369: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 372: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 373: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 374: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 375: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 376: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 377: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 378: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 378: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 380: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 381: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 382: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 383: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 384: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 384: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 386: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 387: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 387: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 387: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 390: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 391: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 392: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 393: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 394: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 395: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 396: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 396: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 398: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 399: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 400: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 401: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 402: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 402: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 404: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 405: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 405: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 405: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 408: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 409: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 410: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 411: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 412: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 413: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 414: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 414: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 416: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 417: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 418: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 419: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 420: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 420: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 422: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 423: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 423: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 423: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 426: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 427: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 428: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 429: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 430: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 431: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 432: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 432: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 434: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 435: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 436: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 437: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 438: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 438: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 440: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 441: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 441: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 441: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 444: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 445: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 446: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 447: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 448: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 449: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 450: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 450: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 452: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 453: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 454: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 455: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 456: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 456: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 458: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 459: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 459: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 459: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 462: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 463: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 464: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 465: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 466: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 467: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 468: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 468: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 470: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 471: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 472: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 473: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 474: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 474: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 476: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 477: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 477: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 477: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 480: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 481: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 482: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 483: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 484: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 485: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 486: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 486: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 488: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 489: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 490: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 491: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 492: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 492: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 494: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 495: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 495: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 495: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 498: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 499: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 500: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 501: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 502: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 503: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 504: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 504: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 506: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 507: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 508: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 509: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 510: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 510: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 512: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 513: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 513: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 513: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 516: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 517: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 518: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 519: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 520: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 521: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 522: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 522: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 524: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 525: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 526: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 527: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 528: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 528: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 530: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 531: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 531: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 531: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 534: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 535: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 536: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 537: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 538: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 539: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 540: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 540: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 542: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 543: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 544: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 545: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 546: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 546: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 548: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 549: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 549: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 549: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 552: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 553: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 554: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 555: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 556: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 557: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 558: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 558: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 560: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 561: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 562: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 563: Linear Output: torch.Size([1, 1, 1024]) Size in MB: 0.00390625\n",
      "Layer 564: LlamaRotaryEmbedding Output 0: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 564: LlamaRotaryEmbedding Output 1: torch.Size([1, 1, 128]) Size in MB: 0.00048828125\n",
      "Layer 566: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 567: LlamaSdpaAttention Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 567: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 567: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 570: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 571: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 572: SiLU Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 573: Linear Output: torch.Size([1, 1, 14336]) Size in MB: 0.0546875\n",
      "Layer 574: Linear Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 575: LlamaMLP Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 576: LlamaDecoderLayer Output 0: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 576: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 578: LlamaRMSNorm Output: torch.Size([1, 1, 4096]) Size in MB: 0.015625\n",
      "Layer 579: LlamaModel Output does not have a 'size' attribute\n",
      "Layer 580: Linear Output: torch.Size([1, 1, 128256]) Size in MB: 0.4892578125\n",
      "Layer 581: LlamaForCausalLM Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "print_model_intermediate_sizes('meta-llama/Meta-Llama-3-8B', size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5630aba-9911-4c62-89fc-4f7a84952fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1fabad070c4ca78dc45df35c1bf190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 2: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 3: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 4: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 5: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 6: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 6: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 8: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 9: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 9: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 9: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 12: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 13: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 14: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 15: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 16: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 17: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 18: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 18: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 20: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 21: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 22: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 23: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 24: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 24: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 26: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 27: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 27: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 27: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 30: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 31: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 32: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 33: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 34: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 35: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 36: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 36: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 38: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 39: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 40: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 41: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 42: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 42: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 44: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 45: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 45: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 45: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 48: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 49: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 50: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 51: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 52: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 53: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 54: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 54: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 56: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 57: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 58: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 59: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 60: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 60: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 62: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 63: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 63: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 63: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 66: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 67: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 68: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 69: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 70: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 71: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 72: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 72: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 74: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 75: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 76: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 77: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 78: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 78: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 80: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 81: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 81: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 81: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 84: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 85: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 86: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 87: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 88: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 89: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 90: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 90: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 92: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 93: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 94: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 95: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 96: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 96: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 98: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 99: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 99: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 99: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 102: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 103: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 104: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 105: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 106: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 107: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 108: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 108: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 110: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 111: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 112: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 113: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 114: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 114: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 116: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 117: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 117: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 117: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 120: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 121: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 122: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 123: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 124: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 125: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 126: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 126: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 128: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 129: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 130: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 131: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 132: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 132: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 134: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 135: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 135: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 135: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 138: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 139: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 140: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 141: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 142: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 143: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 144: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 144: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 146: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 147: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 148: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 149: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 150: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 150: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 152: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 153: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 153: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 153: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 156: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 157: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 158: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 159: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 160: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 161: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 162: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 162: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 164: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 165: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 166: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 167: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 168: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 168: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 170: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 171: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 171: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 171: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 174: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 175: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 176: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 177: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 178: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 179: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 180: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 180: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 182: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 183: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 184: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 185: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 186: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 186: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 188: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 189: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 189: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 189: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 192: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 193: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 194: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 195: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 196: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 197: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 198: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 198: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 200: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 201: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 202: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 203: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 204: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 204: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 206: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 207: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 207: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 207: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 210: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 211: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 212: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 213: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 214: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 215: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 216: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 216: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 218: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 219: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 220: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 221: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 222: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 222: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 224: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 225: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 225: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 225: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 228: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 229: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 230: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 231: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 232: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 233: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 234: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 234: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 236: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 237: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 238: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 239: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 240: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 240: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 242: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 243: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 243: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 243: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 246: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 247: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 248: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 249: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 250: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 251: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 252: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 252: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 254: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 255: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 256: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 257: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 258: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 258: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 260: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 261: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 261: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 261: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 264: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 265: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 266: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 267: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 268: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 269: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 270: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 270: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 272: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 273: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 274: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 275: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 276: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 276: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 278: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 279: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 279: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 279: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 282: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 283: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 284: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 285: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 286: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 287: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 288: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 288: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 290: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 291: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 292: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 293: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 294: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 294: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 296: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 297: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 297: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 297: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 300: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 301: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 302: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 303: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 304: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 305: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 306: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 306: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 308: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 309: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 310: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 311: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 312: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 312: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 314: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 315: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 315: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 315: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 318: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 319: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 320: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 321: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 322: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 323: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 324: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 324: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 326: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 327: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 328: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 329: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 330: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 330: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 332: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 333: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 333: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 333: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 336: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 337: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 338: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 339: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 340: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 341: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 342: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 342: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 344: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 345: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 346: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 347: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 348: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 348: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 350: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 351: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 351: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 351: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 354: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 355: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 356: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 357: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 358: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 359: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 360: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 360: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 362: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 363: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 364: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 365: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 366: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 366: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 368: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 369: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 369: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 369: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 372: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 373: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 374: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 375: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 376: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 377: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 378: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 378: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 380: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 381: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 382: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 383: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 384: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 384: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 386: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 387: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 387: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 387: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 390: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 391: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 392: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 393: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 394: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 395: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 396: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 396: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 398: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 399: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 400: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 401: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 402: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 402: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 404: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 405: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 405: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 405: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 408: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 409: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 410: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 411: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 412: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 413: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 414: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 414: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 416: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 417: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 418: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 419: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 420: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 420: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 422: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 423: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 423: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 423: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 426: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 427: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 428: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 429: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 430: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 431: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 432: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 432: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 434: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 435: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 436: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 437: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 438: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 438: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 440: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 441: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 441: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 441: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 444: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 445: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 446: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 447: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 448: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 449: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 450: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 450: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 452: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 453: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 454: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 455: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 456: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 456: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 458: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 459: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 459: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 459: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 462: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 463: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 464: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 465: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 466: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 467: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 468: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 468: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 470: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 471: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 472: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 473: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 474: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 474: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 476: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 477: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 477: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 477: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 480: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 481: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 482: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 483: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 484: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 485: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 486: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 486: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 488: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 489: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 490: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 491: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 492: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 492: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 494: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 495: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 495: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 495: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 498: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 499: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 500: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 501: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 502: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 503: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 504: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 504: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 506: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 507: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 508: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 509: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 510: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 510: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 512: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 513: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 513: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 513: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 516: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 517: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 518: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 519: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 520: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 521: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 522: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 522: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 524: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 525: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 526: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 527: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 528: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 528: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 530: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 531: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 531: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 531: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 534: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 535: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 536: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 537: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 538: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 539: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 540: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 540: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 542: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 543: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 544: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 545: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 546: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 546: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 548: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 549: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 549: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 549: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 552: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 553: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 554: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 555: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 556: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 557: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 558: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 558: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 560: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 561: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 562: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 563: Linear Output: torch.Size([1, 100, 1024]) Size in MB: 0.390625\n",
      "Layer 564: LlamaRotaryEmbedding Output 0: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 564: LlamaRotaryEmbedding Output 1: torch.Size([1, 100, 128]) Size in MB: 0.048828125\n",
      "Layer 566: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 567: LlamaSdpaAttention Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 567: LlamaSdpaAttention Output 1: Output does not have a 'size' attribute\n",
      "Layer 567: LlamaSdpaAttention Output 2: Output does not have a 'size' attribute\n",
      "Layer 570: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 571: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 572: SiLU Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 573: Linear Output: torch.Size([1, 100, 14336]) Size in MB: 5.46875\n",
      "Layer 574: Linear Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 575: LlamaMLP Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 576: LlamaDecoderLayer Output 0: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 576: LlamaDecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 578: LlamaRMSNorm Output: torch.Size([1, 100, 4096]) Size in MB: 1.5625\n",
      "Layer 579: LlamaModel Output does not have a 'size' attribute\n",
      "Layer 580: Linear Output: torch.Size([1, 100, 128256]) Size in MB: 48.92578125\n",
      "Layer 581: LlamaForCausalLM Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "print_model_intermediate_sizes('meta-llama/Meta-Llama-3-8B', size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "039b9600-01ac-44ce-aaa4-be234cbd6875",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bae23232264a3ba6a487580d7d8738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight size: 2004.00 MB\n",
      "model.layers.0.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.0.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.0.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.0.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.0.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.0.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.0.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.0.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.0.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.1.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.1.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.1.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.1.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.1.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.1.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.1.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.1.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.1.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.2.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.2.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.2.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.2.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.2.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.2.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.2.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.2.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.2.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.3.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.3.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.3.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.3.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.3.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.3.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.3.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.3.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.3.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.4.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.4.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.4.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.4.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.4.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.4.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.4.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.4.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.4.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.5.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.5.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.5.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.5.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.5.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.5.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.5.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.5.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.5.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.6.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.6.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.6.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.6.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.6.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.6.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.6.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.6.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.6.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.7.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.7.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.7.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.7.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.7.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.7.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.7.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.7.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.7.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.8.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.8.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.8.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.8.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.8.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.8.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.8.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.8.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.8.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.9.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.9.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.9.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.9.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.9.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.9.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.9.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.9.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.9.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.10.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.10.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.10.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.10.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.10.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.10.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.10.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.10.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.10.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.11.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.11.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.11.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.11.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.11.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.11.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.11.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.11.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.11.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.12.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.12.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.12.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.12.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.12.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.12.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.12.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.12.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.12.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.13.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.13.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.13.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.13.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.13.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.13.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.13.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.13.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.13.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.14.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.14.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.14.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.14.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.14.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.14.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.14.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.14.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.14.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.15.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.15.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.15.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.15.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.15.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.15.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.15.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.15.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.15.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.16.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.16.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.16.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.16.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.16.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.16.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.16.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.16.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.16.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.17.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.17.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.17.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.17.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.17.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.17.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.17.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.17.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.17.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.18.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.18.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.18.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.18.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.18.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.18.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.18.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.18.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.18.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.19.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.19.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.19.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.19.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.19.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.19.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.19.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.19.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.19.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.20.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.20.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.20.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.20.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.20.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.20.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.20.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.20.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.20.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.21.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.21.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.21.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.21.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.21.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.21.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.21.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.21.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.21.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.22.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.22.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.22.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.22.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.22.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.22.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.22.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.22.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.22.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.23.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.23.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.23.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.23.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.23.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.23.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.23.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.23.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.23.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.24.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.24.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.24.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.24.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.24.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.24.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.24.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.24.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.24.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.25.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.25.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.25.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.25.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.25.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.25.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.25.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.25.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.25.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.26.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.26.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.26.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.26.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.26.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.26.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.26.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.26.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.26.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.27.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.27.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.27.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.27.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.27.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.27.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.27.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.27.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.27.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.28.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.28.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.28.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.28.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.28.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.28.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.28.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.28.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.28.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.29.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.29.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.29.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.29.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.29.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.29.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.29.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.29.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.29.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.30.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.30.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.30.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.30.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.30.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.30.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.30.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.30.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.30.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.layers.31.self_attn.q_proj.weight size: 64.00 MB\n",
      "model.layers.31.self_attn.k_proj.weight size: 16.00 MB\n",
      "model.layers.31.self_attn.v_proj.weight size: 16.00 MB\n",
      "model.layers.31.self_attn.o_proj.weight size: 64.00 MB\n",
      "model.layers.31.mlp.gate_proj.weight size: 224.00 MB\n",
      "model.layers.31.mlp.up_proj.weight size: 224.00 MB\n",
      "model.layers.31.mlp.down_proj.weight size: 224.00 MB\n",
      "model.layers.31.input_layernorm.weight size: 0.02 MB\n",
      "model.layers.31.post_attention_layernorm.weight size: 0.02 MB\n",
      "model.norm.weight size: 0.02 MB\n",
      "lm_head.weight size: 2004.00 MB\n",
      "Total model size: 29.92 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_weights_size('meta-llama/Meta-Llama-3-8B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b43350b-8030-486d-8b69-baaf37748b74",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052e6df986f349c782df3f8dfc978ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for microsoft/Phi-3-mini-128k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-128k-instruct.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e03b5077ad947f3b1f55f258598e166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for microsoft/Phi-3-mini-128k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-128k-instruct.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a52d2434374d77a064bc25f02b586a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7e0780a42b425ebb9553fda95fbf8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfcf11f316a4befaa22ffacf3e95a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02779fef46f4d7ab17b5ac903c2e9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f38d833808d4966b8167e5c8ac1ea94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066602631f4d4ba4b61f45cd28da442f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a825281c47ca40f3bf4ba7d47445f4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 2: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 3: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 4: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 4: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 6: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 7: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 7: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 7: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 10: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 11: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 12: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 13: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 14: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 15: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 16: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 17: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 17: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 19: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 20: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 21: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 21: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 23: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 24: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 24: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 24: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 27: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 28: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 29: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 30: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 31: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 32: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 33: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 34: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 34: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 36: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 37: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 38: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 38: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 40: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 41: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 41: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 41: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 44: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 45: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 46: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 47: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 48: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 49: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 50: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 51: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 51: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 53: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 54: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 55: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 55: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 57: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 58: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 58: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 58: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 61: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 62: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 63: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 64: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 65: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 66: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 67: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 68: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 68: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 70: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 71: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 72: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 72: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 74: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 75: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 75: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 75: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 78: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 79: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 80: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 81: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 82: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 83: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 84: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 85: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 85: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 87: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 88: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 89: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 89: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 91: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 92: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 92: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 92: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 95: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 96: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 97: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 98: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 99: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 100: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 101: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 102: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 102: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 104: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 105: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 106: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 106: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 108: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 109: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 109: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 109: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 112: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 113: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 114: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 115: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 116: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 117: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 118: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 119: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 119: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 121: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 122: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 123: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 123: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 125: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 126: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 126: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 126: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 129: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 130: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 131: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 132: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 133: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 134: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 135: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 136: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 136: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 138: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 139: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 140: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 140: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 142: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 143: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 143: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 143: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 146: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 147: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 148: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 149: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 150: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 151: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 152: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 153: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 153: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 155: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 156: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 157: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 157: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 159: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 160: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 160: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 160: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 163: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 164: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 165: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 166: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 167: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 168: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 169: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 170: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 170: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 172: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 173: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 174: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 174: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 176: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 177: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 177: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 177: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 180: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 181: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 182: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 183: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 184: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 185: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 186: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 187: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 187: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 189: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 190: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 191: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 191: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 193: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 194: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 194: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 194: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 197: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 198: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 199: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 200: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 201: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 202: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 203: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 204: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 204: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 206: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 207: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 208: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 208: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 210: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 211: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 211: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 211: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 214: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 215: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 216: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 217: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 218: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 219: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 220: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 221: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 221: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 223: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 224: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 225: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 225: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 227: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 228: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 228: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 228: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 231: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 232: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 233: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 234: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 235: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 236: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 237: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 238: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 238: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 240: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 241: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 242: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 242: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 244: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 245: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 245: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 245: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 248: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 249: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 250: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 251: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 252: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 253: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 254: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 255: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 255: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 257: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 258: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 259: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 259: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 261: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 262: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 262: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 262: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 265: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 266: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 267: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 268: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 269: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 270: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 271: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 272: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 272: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 274: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 275: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 276: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 276: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 278: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 279: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 279: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 279: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 282: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 283: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 284: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 285: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 286: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 287: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 288: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 289: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 289: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 291: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 292: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 293: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 293: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 295: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 296: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 296: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 296: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 299: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 300: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 301: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 302: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 303: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 304: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 305: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 306: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 306: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 308: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 309: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 310: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 310: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 312: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 313: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 313: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 313: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 316: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 317: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 318: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 319: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 320: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 321: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 322: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 323: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 323: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 325: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 326: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 327: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 327: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 329: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 330: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 330: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 330: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 333: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 334: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 335: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 336: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 337: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 338: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 339: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 340: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 340: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 342: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 343: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 344: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 344: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 346: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 347: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 347: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 347: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 350: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 351: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 352: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 353: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 354: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 355: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 356: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 357: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 357: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 359: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 360: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 361: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 361: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 363: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 364: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 364: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 364: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 367: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 368: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 369: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 370: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 371: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 372: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 373: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 374: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 374: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 376: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 377: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 378: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 378: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 380: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 381: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 381: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 381: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 384: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 385: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 386: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 387: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 388: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 389: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 390: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 391: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 391: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 393: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 394: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 395: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 395: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 397: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 398: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 398: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 398: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 401: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 402: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 403: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 404: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 405: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 406: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 407: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 408: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 408: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 410: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 411: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 412: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 412: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 414: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 415: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 415: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 415: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 418: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 419: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 420: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 421: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 422: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 423: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 424: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 425: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 425: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 427: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 428: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 429: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 429: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 431: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 432: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 432: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 432: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 435: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 436: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 437: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 438: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 439: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 440: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 441: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 442: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 442: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 444: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 445: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 446: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 446: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 448: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 449: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 449: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 449: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 452: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 453: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 454: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 455: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 456: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 457: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 458: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 459: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 459: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 461: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 462: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 463: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 463: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 465: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 466: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 466: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 466: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 469: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 470: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 471: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 472: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 473: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 474: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 475: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 476: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 476: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 478: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 479: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 480: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 480: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 482: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 483: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 483: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 483: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 486: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 487: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 488: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 489: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 490: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 491: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 492: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 493: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 493: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 495: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 496: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 497: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 497: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 499: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 500: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 500: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 500: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 503: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 504: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 505: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 506: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 507: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 508: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 509: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 510: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 510: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 512: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 513: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 514: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 514: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 516: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 517: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 517: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 517: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 520: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 521: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 522: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 523: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 524: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 525: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 526: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 527: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 527: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 529: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 530: Linear Output: torch.Size([1, 1, 9216]) Size in MB: 0.03515625\n",
      "Layer 531: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 531: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 1, 96]) Size in MB: 0.0003662109375\n",
      "Layer 533: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 534: Phi3Attention Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 534: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 534: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 537: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 538: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 539: Linear Output: torch.Size([1, 1, 16384]) Size in MB: 0.0625\n",
      "Layer 540: SiLU Output: torch.Size([1, 1, 8192]) Size in MB: 0.03125\n",
      "Layer 541: Linear Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 542: Phi3MLP Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 543: Dropout Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 544: Phi3DecoderLayer Output 0: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 544: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 546: Phi3RMSNorm Output: torch.Size([1, 1, 3072]) Size in MB: 0.01171875\n",
      "Layer 547: Phi3Model Output does not have a 'size' attribute\n",
      "Layer 548: Linear Output: torch.Size([1, 1, 32064]) Size in MB: 0.122314453125\n",
      "Layer 549: Phi3ForCausalLM Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "print_model_intermediate_sizes('microsoft/Phi-3-mini-128k-instruct', size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959a448a-cada-4f06-8bd5-4970f6dbaed1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for microsoft/Phi-3-mini-128k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-128k-instruct.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d05de1157f46e78ba6fd3afb1acda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight size: 375.75 MB\n",
      "model.layers.0.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.0.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.0.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.0.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.0.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.0.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.1.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.1.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.1.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.1.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.1.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.1.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.2.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.2.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.2.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.2.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.2.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.2.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.3.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.3.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.3.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.3.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.3.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.3.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.4.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.4.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.4.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.4.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.4.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.4.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.5.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.5.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.5.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.5.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.5.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.5.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.6.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.6.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.6.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.6.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.6.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.6.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.7.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.7.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.7.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.7.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.7.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.7.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.8.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.8.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.8.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.8.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.8.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.8.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.9.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.9.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.9.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.9.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.9.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.9.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.10.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.10.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.10.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.10.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.10.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.10.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.11.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.11.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.11.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.11.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.11.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.11.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.12.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.12.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.12.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.12.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.12.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.12.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.13.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.13.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.13.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.13.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.13.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.13.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.14.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.14.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.14.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.14.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.14.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.14.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.15.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.15.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.15.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.15.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.15.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.15.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.16.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.16.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.16.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.16.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.16.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.16.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.17.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.17.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.17.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.17.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.17.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.17.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.18.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.18.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.18.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.18.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.18.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.18.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.19.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.19.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.19.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.19.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.19.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.19.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.20.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.20.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.20.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.20.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.20.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.20.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.21.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.21.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.21.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.21.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.21.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.21.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.22.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.22.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.22.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.22.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.22.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.22.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.23.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.23.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.23.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.23.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.23.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.23.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.24.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.24.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.24.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.24.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.24.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.24.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.25.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.25.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.25.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.25.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.25.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.25.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.26.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.26.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.26.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.26.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.26.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.26.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.27.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.27.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.27.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.27.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.27.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.27.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.28.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.28.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.28.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.28.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.28.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.28.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.29.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.29.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.29.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.29.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.29.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.29.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.30.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.30.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.30.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.30.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.30.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.30.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.layers.31.self_attn.o_proj.weight size: 36.00 MB\n",
      "model.layers.31.self_attn.qkv_proj.weight size: 108.00 MB\n",
      "model.layers.31.mlp.gate_up_proj.weight size: 192.00 MB\n",
      "model.layers.31.mlp.down_proj.weight size: 96.00 MB\n",
      "model.layers.31.input_layernorm.weight size: 0.01 MB\n",
      "model.layers.31.post_attention_layernorm.weight size: 0.01 MB\n",
      "model.norm.weight size: 0.01 MB\n",
      "lm_head.weight size: 375.75 MB\n",
      "Total model size: 14.23 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_weights_size('microsoft/Phi-3-mini-128k-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3303d293-6e7a-4450-bbd8-110e4c05392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for microsoft/Phi-3-mini-128k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-128k-instruct.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29422d1038244048c7e1932c8532f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Embedding Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 2: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 3: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 4: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 4: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 6: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 7: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 7: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 7: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 10: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 11: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 12: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 13: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 14: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 15: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 16: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 17: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 17: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 19: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 20: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 21: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 21: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 23: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 24: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 24: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 24: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 27: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 28: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 29: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 30: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 31: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 32: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 33: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 34: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 34: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 36: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 37: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 38: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 38: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 40: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 41: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 41: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 41: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 44: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 45: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 46: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 47: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 48: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 49: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 50: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 51: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 51: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 53: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 54: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 55: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 55: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 57: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 58: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 58: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 58: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 61: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 62: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 63: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 64: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 65: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 66: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 67: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 68: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 68: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 70: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 71: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 72: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 72: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 74: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 75: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 75: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 75: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 78: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 79: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 80: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 81: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 82: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 83: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 84: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 85: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 85: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 87: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 88: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 89: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 89: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 91: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 92: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 92: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 92: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 95: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 96: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 97: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 98: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 99: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 100: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 101: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 102: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 102: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 104: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 105: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 106: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 106: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 108: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 109: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 109: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 109: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 112: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 113: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 114: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 115: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 116: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 117: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 118: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 119: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 119: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 121: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 122: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 123: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 123: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 125: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 126: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 126: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 126: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 129: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 130: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 131: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 132: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 133: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 134: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 135: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 136: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 136: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 138: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 139: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 140: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 140: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 142: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 143: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 143: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 143: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 146: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 147: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 148: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 149: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 150: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 151: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 152: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 153: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 153: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 155: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 156: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 157: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 157: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 159: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 160: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 160: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 160: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 163: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 164: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 165: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 166: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 167: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 168: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 169: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 170: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 170: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 172: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 173: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 174: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 174: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 176: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 177: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 177: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 177: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 180: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 181: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 182: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 183: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 184: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 185: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 186: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 187: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 187: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 189: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 190: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 191: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 191: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 193: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 194: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 194: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 194: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 197: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 198: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 199: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 200: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 201: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 202: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 203: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 204: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 204: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 206: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 207: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 208: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 208: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 210: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 211: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 211: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 211: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 214: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 215: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 216: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 217: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 218: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 219: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 220: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 221: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 221: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 223: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 224: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 225: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 225: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 227: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 228: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 228: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 228: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 231: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 232: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 233: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 234: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 235: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 236: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 237: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 238: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 238: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 240: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 241: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 242: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 242: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 244: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 245: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 245: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 245: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 248: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 249: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 250: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 251: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 252: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 253: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 254: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 255: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 255: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 257: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 258: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 259: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 259: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 261: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 262: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 262: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 262: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 265: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 266: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 267: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 268: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 269: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 270: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 271: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 272: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 272: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 274: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 275: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 276: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 276: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 278: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 279: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 279: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 279: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 282: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 283: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 284: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 285: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 286: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 287: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 288: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 289: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 289: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 291: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 292: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 293: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 293: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 295: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 296: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 296: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 296: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 299: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 300: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 301: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 302: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 303: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 304: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 305: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 306: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 306: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 308: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 309: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 310: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 310: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 312: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 313: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 313: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 313: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 316: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 317: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 318: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 319: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 320: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 321: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 322: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 323: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 323: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 325: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 326: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 327: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 327: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 329: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 330: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 330: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 330: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 333: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 334: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 335: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 336: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 337: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 338: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 339: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 340: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 340: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 342: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 343: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 344: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 344: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 346: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 347: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 347: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 347: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 350: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 351: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 352: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 353: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 354: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 355: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 356: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 357: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 357: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 359: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 360: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 361: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 361: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 363: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 364: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 364: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 364: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 367: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 368: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 369: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 370: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 371: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 372: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 373: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 374: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 374: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 376: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 377: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 378: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 378: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 380: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 381: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 381: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 381: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 384: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 385: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 386: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 387: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 388: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 389: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 390: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 391: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 391: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 393: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 394: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 395: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 395: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 397: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 398: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 398: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 398: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 401: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 402: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 403: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 404: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 405: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 406: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 407: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 408: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 408: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 410: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 411: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 412: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 412: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 414: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 415: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 415: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 415: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 418: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 419: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 420: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 421: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 422: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 423: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 424: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 425: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 425: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 427: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 428: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 429: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 429: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 431: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 432: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 432: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 432: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 435: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 436: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 437: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 438: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 439: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 440: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 441: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 442: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 442: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 444: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 445: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 446: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 446: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 448: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 449: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 449: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 449: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 452: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 453: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 454: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 455: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 456: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 457: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 458: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 459: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 459: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 461: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 462: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 463: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 463: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 465: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 466: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 466: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 466: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 469: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 470: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 471: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 472: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 473: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 474: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 475: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 476: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 476: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 478: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 479: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 480: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 480: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 482: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 483: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 483: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 483: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 486: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 487: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 488: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 489: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 490: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 491: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 492: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 493: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 493: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 495: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 496: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 497: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 497: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 499: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 500: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 500: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 500: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 503: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 504: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 505: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 506: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 507: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 508: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 509: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 510: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 510: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 512: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 513: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 514: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 514: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 516: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 517: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 517: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 517: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 520: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 521: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 522: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 523: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 524: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 525: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 526: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 527: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 527: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 529: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 530: Linear Output: torch.Size([1, 100, 9216]) Size in MB: 3.515625\n",
      "Layer 531: Phi3SuScaledRotaryEmbedding Output 0: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 531: Phi3SuScaledRotaryEmbedding Output 1: torch.Size([1, 100, 96]) Size in MB: 0.03662109375\n",
      "Layer 533: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 534: Phi3Attention Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 534: Phi3Attention Output 1: Output does not have a 'size' attribute\n",
      "Layer 534: Phi3Attention Output 2: Output does not have a 'size' attribute\n",
      "Layer 537: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 538: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 539: Linear Output: torch.Size([1, 100, 16384]) Size in MB: 6.25\n",
      "Layer 540: SiLU Output: torch.Size([1, 100, 8192]) Size in MB: 3.125\n",
      "Layer 541: Linear Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 542: Phi3MLP Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 543: Dropout Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 544: Phi3DecoderLayer Output 0: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 544: Phi3DecoderLayer Output 1: Output does not have a 'size' attribute\n",
      "Layer 546: Phi3RMSNorm Output: torch.Size([1, 100, 3072]) Size in MB: 1.171875\n",
      "Layer 547: Phi3Model Output does not have a 'size' attribute\n",
      "Layer 548: Linear Output: torch.Size([1, 100, 32064]) Size in MB: 12.2314453125\n",
      "Layer 549: Phi3ForCausalLM Output does not have a 'size' attribute\n"
     ]
    }
   ],
   "source": [
    "print_model_intermediate_sizes('microsoft/Phi-3-mini-128k-instruct', size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000631a-9af6-46d1-9f1e-894a9519e8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
