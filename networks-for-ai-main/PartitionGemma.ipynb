{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d824f72d-176d-4b67-aee4-c4487d0abeea",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf842e90-4a37-4e3f-a4d4-0af165bbf916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "from tools.config import get_config_for_7b, get_config_for_2b\n",
    "import tools.config as gemma_config\n",
    "from tools.model import GemmaForCausalLM, GemmaDecoderLayer, RMSNorm, Sampler, Embedding, precompute_freqs_cis\n",
    "from tools.tokenizer import Tokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import Any, List, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b213d04-ef6f-4f4c-a2b7-34df787cb1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0536d23823b04cdd90170536b26829b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose variant and machine type\n",
    "VARIANT = '1.1-7b-it'\n",
    "MACHINE_TYPE = 'cuda'\n",
    "\n",
    "kagglehub.login() # 5cb66339276d4bea7ba59ca714d28f6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4179b751-8d36-4fb8-989f-891d9e1acc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaFirstLayerModel(nn.Module):\n",
    "    def __init__(self, config: gemma_config.GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.first_layer = GemmaDecoderLayer(config)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, freqs_cis: torch.Tensor, kv_write_indices: torch.Tensor, kv_cache: Tuple[torch.Tensor, torch.Tensor], mask: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.first_layer(hidden_states=hidden_states, freqs_cis=freqs_cis, kv_write_indices=kv_write_indices, kv_cache=kv_cache, mask=mask)\n",
    "        return hidden_states\n",
    "\n",
    "class GemmaRemainingLayersModel(nn.Module):\n",
    "    def __init__(self, config: gemma_config.GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([GemmaDecoderLayer(config) for _ in range(1, config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, freqs_cis: torch.Tensor, kv_write_indices: torch.Tensor, kv_caches: List[Tuple[torch.Tensor, torch.Tensor]], mask: torch.Tensor) -> torch.Tensor:\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            hidden_states = layer(hidden_states=hidden_states, freqs_cis=freqs_cis, kv_write_indices=kv_write_indices, kv_cache=kv_caches[i], mask=mask)\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "class GemmaMiddleLayersModel(nn.Module):\n",
    "    def __init__(self, config: gemma_config.GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([GemmaDecoderLayer(config) for _ in range(1, config.num_hidden_layers - 1)])\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, freqs_cis: torch.Tensor, kv_write_indices: torch.Tensor, kv_caches: List[Tuple[torch.Tensor, torch.Tensor]], mask: torch.Tensor) -> torch.Tensor:\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            hidden_states = layer(hidden_states=hidden_states, freqs_cis=freqs_cis, kv_write_indices=kv_write_indices, kv_cache=kv_caches[i], mask=mask)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe752dd7-038b-4561-8ab4-1bd336079276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, filepath):\n",
    "    \"\"\"Loads a model's state dictionary from a specified filepath.\"\"\"\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    \n",
    "def print_MB_size(output, factor=1):\n",
    "    output = output\n",
    "    num_elements = output.numel()\n",
    "    element_size = output.element_size()  # Returns the size in bytes of each element\n",
    "    total_memory_MB = num_elements * element_size / 1024**2\n",
    "    return f\"Size in MB: {total_memory_MB*factor}\"\n",
    "    \n",
    "def load_weights(model, model_path: str):\n",
    "    \"\"\"Original function used for weight loading in GemmaForCausalLM\"\"\"\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            model_path, mmap=True, weights_only=True,\n",
    "        )['model_state_dict'],\n",
    "        strict=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b56d4-36fd-49e3-a9a0-b5ea7cb6de4d",
   "metadata": {},
   "source": [
    "If you have already partitioned the model or have access to the partitioned models weights skip until [Model split inference](## Model split inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585550c6-e43f-48fa-97d5-03f9a90a49a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Full Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d74e6c1-865a-49c1-9124-3465867b3113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.2.5)\n",
      "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/pyTorch/1.1-7b-it/1/download...\n",
      "100%|██████████| 12.7G/12.7G [02:09<00:00, 105MB/s] \n",
      "Extracting model files...\n"
     ]
    }
   ],
   "source": [
    "# Load model weights\n",
    "weights_dir = kagglehub.model_download(f'google/gemma/pyTorch/{VARIANT}')\n",
    "\n",
    "# Ensure that the tokenizer is present\n",
    "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
    "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
    "\n",
    "# Ensure that the checkpoint is present\n",
    "ckpt_path = os.path.join(weights_dir, f'gemma-{\"-\".join(VARIANT.split(\"-\")[1:])}.ckpt')\n",
    "assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "696f39a5-9b55-44ec-aee0-932e0ea9b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# Set up model config.\n",
    "model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "model_config.tokenizer = tokenizer_path\n",
    "model_config.quant = 'quant' in VARIANT\n",
    "\n",
    "# Instantiate the model and load the weights.\n",
    "torch.set_default_dtype(model_config.get_dtype())\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "model = GemmaForCausalLM(model_config)\n",
    "model.load_weights(ckpt_path)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3de2a3-0e58-457d-897d-124245725cef",
   "metadata": {},
   "source": [
    "Example of full model usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d804104-f79a-4e01-b0d9-36ad90864b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat prompt:\n",
      " <start_of_turn>user\n",
      "What is the best city in Europe?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Barcelona.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "What can I do in Barcelona?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"## Things you can do in Barcelona:\\n\\n**Culture & History:**\\n\\n* Explore the Gothic Quarter, with its narrow streets and medieval architecture.\\n* Visit the Sagrada Familia, Gaudi's unfinished masterpiece.\\n* Marvel at Parc Guell, another iconic Gaudi creation with stunning city views.\\n* Learn about the history of FC Barcelona at the Camp Nou stadium.\\n* Explore the history of the city at the Barcelona History Museum.\\n\\n**Food & Drink:**\\n\\n* Sample tapas\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate with one request in chat mode\n",
    "\n",
    "# Chat templates\n",
    "USER_CHAT_TEMPLATE = '<start_of_turn>user\\n{prompt}<end_of_turn>\\n'\n",
    "MODEL_CHAT_TEMPLATE = '<start_of_turn>model\\n{prompt}<end_of_turn>\\n'\n",
    "\n",
    "# Sample formatted prompt\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(\n",
    "        prompt='What is the best city in Europe?'\n",
    "    )\n",
    "    + MODEL_CHAT_TEMPLATE.format(prompt='Barcelona.')\n",
    "    + USER_CHAT_TEMPLATE.format(prompt='What can I do in Barcelona?')\n",
    "    + '<start_of_turn>model\\n'\n",
    ")\n",
    "print('Chat prompt:\\n', prompt)\n",
    "\n",
    "model.generate(\n",
    "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
    "    device=device,\n",
    "    output_len=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e848620-ead4-41d5-a6c9-83301bd61abb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46cdf70-e8a5-46bf-9c78-cb7a9f8689ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "# Initialize and load weights for both models\n",
    "first_layer_model = GemmaFirstLayerModel(config)\n",
    "remaining_layers_model = GemmaRemainingLayersModel(config)\n",
    "middle_layers_model = GemmaMiddleLayersModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6642b3d6-75e0-4eea-8066-6a95730e7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original model to be splitted\n",
    "model = GemmaForCausalLM(model_config)\n",
    "model.load_weights(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "507be7ef-1c97-45c0-93c0-0d0f9a57fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_keys_for_first_layer(state_dict):\n",
    "    \"\"\"Adjusts the state_dict's keys for the first layer model.\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('model.layers.0.'):\n",
    "            new_key = key.replace('model.layers.0.', 'first_layer.')\n",
    "            new_state_dict[new_key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "def adjust_keys_for_remaining_layers(state_dict):\n",
    "    \"\"\"Adjusts the state_dict's keys for the remaining layers model.\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('model.layers.'):\n",
    "            # Shift layers down by one to account for the first layer being separate\n",
    "            layer_num = int(key.split('.')[2]) - 1\n",
    "            if layer_num >= 0:  # Ensure we don't include the first layer\n",
    "                new_key = 'layers.' + str(layer_num) + '.' + '.'.join(key.split('.')[3:])\n",
    "                new_state_dict[new_key] = value\n",
    "        elif key.startswith('model.norm.weight'): \n",
    "            new_state_dict['norm.weight'] = value\n",
    "            \n",
    "    return new_state_dict\n",
    "\n",
    "def adjust_keys_for_middle_layers(state_dict, config):\n",
    "    \"\"\"Adjusts the state_dict's keys for the remaining layers model.\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('model.layers.'):\n",
    "            # Shift layers down by one to account for the first layer being separate\n",
    "            layer_num = int(key.split('.')[2]) - 1\n",
    "            if layer_num >= 0 and layer_num <= config.num_hidden_layers-1:  # Ensure we don't include the first layer or last\n",
    "                new_key = 'layers.' + str(layer_num) + '.' + '.'.join(key.split('.')[3:])\n",
    "                new_state_dict[new_key] = value            \n",
    "    return new_state_dict\n",
    "\n",
    "def load_adjusted_weights(model, adjusted_state_dict):\n",
    "    \"\"\"Loads the adjusted weights into the model, with strict=False to allow for incomplete state dicts.\"\"\"\n",
    "    model.load_state_dict(adjusted_state_dict, strict=False)\n",
    "    \n",
    "def save_model(model, filepath):\n",
    "    \"\"\"Saves a model's state dictionary to a specified filepath.\"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c436b9-0626-4522-a7e0-ff78f66bbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_state_dict is the state dict loaded from the complete GemmaForCausalLM model\n",
    "full_model_state_dict = model.state_dict()\n",
    "# Adjust and load weights for the first layer model\n",
    "first_layer_state_dict = adjust_keys_for_first_layer(full_model_state_dict)\n",
    "load_adjusted_weights(first_layer_model, first_layer_state_dict)\n",
    "\n",
    "# Adjust and load weights for the remaining layers model\n",
    "remaining_layers_state_dict = adjust_keys_for_remaining_layers(full_model_state_dict)\n",
    "load_adjusted_weights(remaining_layers_model, remaining_layers_state_dict)\n",
    "\n",
    "# Adjust and load weights for the middle layers model\n",
    "middle_layers_state_dict = adjust_keys_for_middle_layers(full_model_state_dict, config)\n",
    "load_adjusted_weights(middle_layers_model, middle_layers_state_dict)\n",
    "\n",
    "# Save the first layer model\n",
    "save_model(first_layer_model, f'./weights/{VARIANT}/first_layer_model.pth')\n",
    "\n",
    "# Save the remaining layers model\n",
    "save_model(remaining_layers_model, f'./weights/{VARIANT}/remaining_layers_model.pth')\n",
    "\n",
    "# Save the remaining layers model\n",
    "save_model(middle_layers_model, f'./weights/{VARIANT}/middle_layers_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20e897da-f93a-413c-b7aa-c3df6c7bd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is the loaded GemmaForCausalLM instance\n",
    "# Save Embedding weights\n",
    "torch.save(model.embedder.state_dict(), f'./weights/{VARIANT}/embedding_weights.pth')\n",
    "\n",
    "# Sampler does not have any trainable parameters or buffers to save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcfd3d6-3e03-4dc4-8918-98dc34894a30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Splitting tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9556e90-48c4-4f72-a8b6-bdb343e630af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(model1, model2):    \n",
    "    for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "        if p1.data.ne(p2.data).sum() > 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4359a8e-ef12-4e2e-a4e2-8e7a1bf4f408",
   "metadata": {},
   "source": [
    "First layer and embedder loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12840db4-7b9d-41c4-aed6-743b43e58f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize empty splits\n",
    "first_layer_model = GemmaFirstLayerModel(model_config)\n",
    "# Load trained weights\n",
    "load_model(first_layer_model, f'./weights/{VARIANT}/first_layer_model.pth')\n",
    "first_layer_model.to(device)\n",
    "# Embedder\n",
    "embedder = Embedding(model_config.vocab_size, model_config.hidden_size, model_config.quant)\n",
    "load_model(embedder, f'./weights/{VARIANT}/embedding_weights.pth')\n",
    "embedder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15f793-e282-4bdc-b35b-cd74d3fe7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d968be-0c19-4e3a-88ce-bc087b3ef04e",
   "metadata": {},
   "source": [
    "Remaining layers and freqs cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd7f7382-99aa-4d94-8364-5f787efdb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_layers_model = GemmaRemainingLayersModel(model_config)\n",
    "load_model(remaining_layers_model, f'./weights/{VARIANT}/remaining_layers_model.pth')\n",
    "# Pre-compute rotary embedding table.\n",
    "rope_theta = getattr(model_config, 'rope_theta', 10000)\n",
    "prec_freqs_cis = precompute_freqs_cis(model_config.head_dim,\n",
    "                                 model_config.max_position_embeddings * 2,\n",
    "                                 theta=rope_theta).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82a14b77-3942-4cd7-942c-c892b1c01aa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layers.0.self_attn.qkv_proj.weight',\n",
       "              tensor([[9.1835e-41, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "                       0.0000e+00],\n",
       "                      [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.1835e-41, 0.0000e+00,\n",
       "                       0.0000e+00],\n",
       "                      [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "                       0.0000e+00],\n",
       "                      ...,\n",
       "                      [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "                       0.0000e+00],\n",
       "                      [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "                       0.0000e+00],\n",
       "                      [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "                       0.0000e+00]])),\n",
       "             ('layers.0.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.0.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.0.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.0.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.0.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.0.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.1.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.1.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.1.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.1.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.1.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.1.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.1.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.2.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.2.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.2.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.2.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.2.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.2.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.2.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.3.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.3.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.3.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.3.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.3.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.3.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.3.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.4.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.4.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.4.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.4.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.4.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.4.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.4.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.5.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.5.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.5.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.5.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.5.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.5.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.5.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.6.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.6.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.6.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.6.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.6.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.6.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.6.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.7.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.7.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.7.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.7.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.7.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.7.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.7.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.8.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.8.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.8.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.8.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.8.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.8.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.8.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.9.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.9.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.9.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.9.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.9.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.9.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.9.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.10.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.10.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.10.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.10.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.10.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.10.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.10.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.11.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.11.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.11.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.11.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.11.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.11.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.11.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.12.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.12.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.12.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.12.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.12.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.12.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.12.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.13.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.13.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.13.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.13.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.13.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.13.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.13.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.14.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.14.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.14.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.14.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.14.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.14.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.14.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.15.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.15.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.15.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.15.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.15.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.15.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.15.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.16.self_attn.qkv_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.16.self_attn.o_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.16.mlp.gate_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.16.mlp.up_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.16.mlp.down_proj.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]])),\n",
       "             ('layers.16.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('layers.16.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('norm.weight', tensor([0., 0., 0.,  ..., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_layers_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b9c7c-3877-456b-988f-07d1785438b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Split model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a723e4-4531-4c83-a08b-67fb31a50818",
   "metadata": {},
   "source": [
    "Tokenizer must be present at the first layer model side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8473878-fe4b-4f49-9082-9837e627138c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the tokenizer is present\n",
    "weights_dir = kagglehub.model_download(f'google/gemma/pyTorch/{VARIANT}')\n",
    "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
    "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d133b866-0990-49e2-a4d0-8bfe67eb7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration\n",
    "MACHINE_TYPE = 'cpu'\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "config.tokenizer = tokenizer_path\n",
    "config.quant = 'quant' in VARIANT\n",
    "torch.set_default_dtype(config.get_dtype())\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "# Restart from saved models\n",
    "# Initialize empty splits\n",
    "first_layer_model = GemmaFirstLayerModel(config) \n",
    "remaining_layers_model = GemmaRemainingLayersModel(config)\n",
    "# Load trained weights\n",
    "load_model(first_layer_model, f'./weights/{VARIANT}/first_layer_model.pth')\n",
    "load_model(remaining_layers_model, f'./weights/{VARIANT}/remaining_layers_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92c7c0-829c-49f0-a73a-c85769dcc55c",
   "metadata": {},
   "source": [
    "Auxiliary structures declaration for inference, originally in GemmaForCausalLM. These will need to be declared at UE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2b3d924-c5f0-4127-8d63-8a8a26ae2e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# Hidden size needs to be divisible by the number of heads\n",
    "assert config.hidden_size % config.num_attention_heads == 0\n",
    "\n",
    "# Prepare hidden_states, freqs_cis, kv_write_indices, kv_cache, and mask for inference\n",
    "max_seq_len = config.max_position_embeddings\n",
    "head_dim = config.head_dim\n",
    "vocab_size = config.vocab_size\n",
    "\n",
    "tokenizer = Tokenizer(config.tokenizer)\n",
    "# Initialize embedder\n",
    "embedder = Embedding(vocab_size, config.hidden_size, config.quant).to(device)\n",
    "embedding_weights = torch.load(f'./weights/{VARIANT}/embedding_weights.pth', map_location=device)\n",
    "embedder.load_state_dict(embedding_weights)\n",
    "model = [first_layer_model.to(device).eval(), remaining_layers_model.to(device).eval()]\n",
    "# Initialize sampler\n",
    "sampler = Sampler(vocab_size).to(device)\n",
    "# Pre-compute rotary embedding table.\n",
    "rope_theta = getattr(config, 'rope_theta', 10000)\n",
    "prec_freqs_cis = precompute_freqs_cis(head_dim,\n",
    "                                 max_seq_len * 2,\n",
    "                                 theta=rope_theta).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c721ce36-9237-4ac5-b929-257b1855a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(\n",
    "    output_index,\n",
    "    input_token_ids: torch.Tensor,\n",
    "    input_positions: torch.Tensor,\n",
    "    kv_write_indices: torch.Tensor,\n",
    "    kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n",
    "    mask: torch.Tensor,\n",
    "    output_positions: torch.Tensor,\n",
    "    temperatures: Union[torch.Tensor, None],\n",
    "    top_ps: torch.Tensor,\n",
    "    top_ks: torch.Tensor,\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "    freqs_cis = prec_freqs_cis.index_select(0, input_positions)\n",
    "    kv_write_indices = input_positions\n",
    "\n",
    "    # [batch_size, input_len, hidden_size]\n",
    "    hidden_states = embedder(input_token_ids)\n",
    "    # Gemma normalizes the embedding by sqrt(hidden_size).\n",
    "    hidden_states = hidden_states * (config.hidden_size**0.5)\n",
    "    with torch.no_grad():\n",
    "        for i in range(config.num_hidden_layers):\n",
    "            if model[i] is None:\n",
    "                continue\n",
    "            hidden_states = model[i](\n",
    "                hidden_states=hidden_states,\n",
    "                freqs_cis=freqs_cis,\n",
    "                kv_write_indices=kv_write_indices,\n",
    "                kv_cache=kv_caches[i],\n",
    "                mask=mask,\n",
    "            )\n",
    "            ########################################################\n",
    "            # print(output_index.item())\n",
    "            # print('Hidden states size: ', print_MB_size(hidden_states))\n",
    "            # print('Freqs cis size: : ', print_MB_size(freqs_cis))\n",
    "            # print('KV Write Indices size: : ', print_MB_size(kv_write_indices))\n",
    "            # print('KV Caches size: ', print_MB_size(kv_caches[1][0], factor=len(kv_caches)*2))\n",
    "            # print('Mask size: ', print_MB_size(mask))\n",
    "            ########################################################\n",
    "    embedder_weight = embedder.weight\n",
    "    if config.quant:\n",
    "        embedder_weight = (\n",
    "            embedder_weight * embedder.weight_scaler.unsqueeze(-1))\n",
    "    next_tokens = sampler(\n",
    "        embedding=embedder_weight,\n",
    "        hidden_states=hidden_states,\n",
    "        output_positions=output_positions,\n",
    "        temperatures=temperatures,\n",
    "        top_ps=top_ps,\n",
    "        top_ks=top_ks,\n",
    "    )\n",
    "    return next_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "511ec089-58de-4bd0-a147-9495a7837d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompts: Union[str, Sequence[str]],\n",
    "    device: Any,\n",
    "    output_len: int = 100,\n",
    "    temperature: Union[float, None] = 0.95,\n",
    "    top_p: float = 1.0,\n",
    "    top_k: int = 100,\n",
    ") -> Union[str, Sequence[str]]:\n",
    "    \"\"\"Generates responses for given prompts using Gemma model.\"\"\"\n",
    "    # If a single prompt is provided, treat it as a batch of 1.\n",
    "    is_str_prompt = isinstance(prompts, str)\n",
    "    if is_str_prompt:\n",
    "        prompts = [prompts]\n",
    "\n",
    "    batch_size = len(prompts)\n",
    "    prompt_tokens = [tokenizer.encode(prompt) for prompt in prompts]\n",
    "    min_prompt_len = min(len(p) for p in prompt_tokens)\n",
    "    max_prompt_len = max(len(p) for p in prompt_tokens)\n",
    "    max_seq_len = max_prompt_len + output_len\n",
    "    assert max_seq_len <= config.max_position_embeddings\n",
    "\n",
    "    # build KV caches\n",
    "    kv_caches = []\n",
    "    for _ in range(config.num_hidden_layers):\n",
    "        size = (batch_size, max_seq_len, config.num_key_value_heads,\n",
    "                config.head_dim)\n",
    "        dtype = config.get_dtype()\n",
    "        k_cache = torch.zeros(size=size, dtype=dtype, device=device)\n",
    "        v_cache = torch.zeros(size=size, dtype=dtype, device=device)\n",
    "        kv_caches.append((k_cache, v_cache))\n",
    "\n",
    "    # prepare inputs\n",
    "    token_ids_tensor = torch.full((batch_size, max_seq_len),\n",
    "                                  tokenizer.pad_id, dtype=torch.int64)\n",
    "    input_token_ids_tensor = torch.full((batch_size, min_prompt_len),\n",
    "                                        tokenizer.pad_id,\n",
    "                                        dtype=torch.int64)\n",
    "    for i, p in enumerate(prompt_tokens):\n",
    "        token_ids_tensor[i, :len(p)] = torch.tensor(p)\n",
    "        input_token_ids_tensor[i, :min_prompt_len] = torch.tensor(\n",
    "            p[:min_prompt_len])\n",
    "    token_ids_tensor = token_ids_tensor.to(device)\n",
    "    input_token_ids_tensor = input_token_ids_tensor.to(device)\n",
    "    prompt_mask_tensor = token_ids_tensor != tokenizer.pad_id\n",
    "    input_positions_tensor = torch.arange(0, min_prompt_len,\n",
    "                                          dtype=torch.int64).to(device)\n",
    "    mask_tensor = torch.full((1, 1, max_seq_len, max_seq_len),\n",
    "                             -2.3819763e38).to(torch.float)\n",
    "    mask_tensor = torch.triu(mask_tensor, diagonal=1).to(device)\n",
    "    curr_mask_tensor = mask_tensor.index_select(2, input_positions_tensor)\n",
    "    output_positions_tensor = torch.LongTensor([min_prompt_len - 1]).to(\n",
    "        device)\n",
    "    temperatures_tensor = None if not temperature else torch.FloatTensor(\n",
    "        [temperature] * batch_size).to(device)\n",
    "    top_ps_tensor = torch.FloatTensor([top_p] * batch_size).to(device)\n",
    "    top_ks_tensor = torch.LongTensor([top_k] * batch_size).to(device)\n",
    "    output_index = torch.tensor(min_prompt_len, dtype=torch.int64).to(\n",
    "        device)\n",
    "\n",
    "    # Prefill up to min_prompt_len tokens, then treat other prefill as\n",
    "    # decode and ignore output.\n",
    "    for i in range(max_seq_len - min_prompt_len):\n",
    "        next_token_ids = forward_pass(\n",
    "            output_index,\n",
    "            input_token_ids=input_token_ids_tensor,\n",
    "            input_positions=input_positions_tensor,\n",
    "            kv_write_indices=None,\n",
    "            kv_caches=kv_caches,\n",
    "            mask=curr_mask_tensor,\n",
    "            output_positions=output_positions_tensor,\n",
    "            temperatures=temperatures_tensor,\n",
    "            top_ps=top_ps_tensor,\n",
    "            top_ks=top_ks_tensor,\n",
    "        )\n",
    "\n",
    "        curr_prompt_mask = prompt_mask_tensor.index_select(\n",
    "            1, output_index).squeeze(dim=1)\n",
    "        curr_token_ids = token_ids_tensor.index_select(\n",
    "            1, output_index).squeeze(dim=1)\n",
    "        output_token_ids = torch.where(curr_prompt_mask, curr_token_ids,\n",
    "                                       next_token_ids).unsqueeze(dim=1)\n",
    "        token_ids_tensor.index_copy_(1, output_index, output_token_ids)\n",
    "\n",
    "        input_token_ids_tensor = output_token_ids\n",
    "        input_positions_tensor = output_index.unsqueeze(dim=-1)\n",
    "        curr_mask_tensor = mask_tensor.index_select(2,\n",
    "                                                    input_positions_tensor)\n",
    "        output_positions_tensor = torch.tensor(0, dtype=torch.int64).to(\n",
    "            device)\n",
    "        output_index = output_index + 1\n",
    "\n",
    "    # Detokenization.\n",
    "    token_ids = token_ids_tensor.tolist()\n",
    "    results = []\n",
    "    for i, tokens in enumerate(token_ids):\n",
    "        trimmed_output = tokens[len(prompt_tokens[i]):len(prompt_tokens[i])\n",
    "                                + output_len]\n",
    "        if tokenizer.eos_id in trimmed_output:\n",
    "            eos_index = trimmed_output.index(tokenizer.eos_id)\n",
    "            trimmed_output = trimmed_output[:eos_index]\n",
    "        results.append(tokenizer.decode(trimmed_output))\n",
    "\n",
    "    # If a string was provided as input, return a string as output.\n",
    "    return results[0] if is_str_prompt else results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56447cfb-ac5d-4de3-94b0-9c5a876f05ff",
   "metadata": {},
   "source": [
    "Use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3d458-e368-4bac-93db-d1f434788d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with one request in chat mode\n",
    "\n",
    "# Chat templates\n",
    "USER_CHAT_TEMPLATE = '<start_of_turn>user\\n{prompt}<end_of_turn>\\n'\n",
    "MODEL_CHAT_TEMPLATE = '<start_of_turn>model\\n{prompt}<end_of_turn>\\n'\n",
    "\n",
    "# Sample formatted prompt\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(\n",
    "        prompt='What is the best city in Europe?'\n",
    "    )\n",
    "    + MODEL_CHAT_TEMPLATE.format(prompt='Barcelona.')\n",
    "    + USER_CHAT_TEMPLATE.format(prompt='What can I do in Barcelona?')\n",
    "    + '<start_of_turn>model\\n'\n",
    ")\n",
    "print('Chat prompt:\\n', prompt)\n",
    "\n",
    "generate(\n",
    "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
    "    device=device,\n",
    "    output_len=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020cc68-40a5-4df9-9936-be8ac0f122b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Tokenizer experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95fb5b6d-b425-4306-9183-3c2029d74142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# Choose variant and machine type\n",
    "VARIANT = '2b-it'\n",
    "MACHINE_TYPE = 'cuda'\n",
    "\n",
    "# Download tokenizer\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "weights_dir = os.path.join(home_dir, \".cache\", \"kagglehub\", \"models\", \"google\", \"gemma\", \"pyTorch\", \"2b-it\", \"2\")\n",
    "if not os.path.exists(weights_dir):\n",
    "    kagglehub.login() # API KEY: 5cb66339276d4bea7ba59ca714d28f6b\n",
    "    weights_dir = kagglehub.model_download(f'google/gemma/pyTorch/{VARIANT}')\n",
    "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
    "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
    "\n",
    "# Define config\n",
    "config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "config.tokenizer = tokenizer_path\n",
    "config.quant = 'quant' in VARIANT\n",
    "torch.set_default_dtype(config.get_dtype())\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "\n",
    "# Hidden size needs to be divisible by the number of heads\n",
    "assert config.hidden_size % config.num_attention_heads == 0\n",
    "\n",
    "# Prepare inference auxiliary structures\n",
    "max_seq_len = config.max_position_embeddings\n",
    "head_dim = config.head_dim\n",
    "vocab_size = config.vocab_size\n",
    "tokenizer = Tokenizer(config.tokenizer)\n",
    "\n",
    "# Chat templates\n",
    "USER_CHAT_TEMPLATE = '<start_of_turn>user\\n{prompt}<end_of_turn>\\n'\n",
    "MODEL_CHAT_TEMPLATE = '<start_of_turn>model\\n{prompt}<end_of_turn>\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda9da46-253d-4336-9d40-9caafba50832",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Northeastern University, with its rich history dating back to 1898, stands as a beacon of innovation nestled in the vibrant city of Boston, Massachusetts. Spanning across its sprawling urban campus, Northeastern embodies a dynamic and diverse community where students from all walks of life converge to embark on transformative educational journeys. At the heart of Northeastern's educational ethos lies a commitment to experiential learning, epitomized by its renowned cooperative education program, which seamlessly integrates classroom theory with real-world practice, empowering students to develop practical skills, gain invaluable industry experience, and forge meaningful connections that lay the foundation for successful careers. As students traverse the bustling streets of Boston, they are enveloped by a myriad of opportunities for personal and intellectual growth, from internships and research initiatives to cultural events and community engagement endeavors. Northeastern's academic offerings are as diverse and expansive as its student body, encompassing a wide array of disciplines ranging from business and engineering to health sciences, arts, and beyond. With world-class faculty at the helm, students are encouraged to explore their passions, pursue interdisciplinary studies, and push the boundaries of knowledge through innovative research and scholarship. Beyond the classroom, Northeastern fosters a vibrant campus culture characterized by a spirit of collaboration, inclusivity, and social responsibility. Through student-led organizations, volunteer projects, and advocacy efforts, students are empowered to effect positive change in their communities and beyond, embodying Northeastern's core values of global citizenship and civic engagement. As a hub of innovation and entrepreneurship, Northeastern serves as a catalyst for economic growth and societal advancement, providing resources, mentorship, and support to aspiring entrepreneurs and innovators. Whether launching a startup, developing groundbreaking technologies, or tackling pressing social issues, Northeastern students and alumni are at the forefront of driving positive change and shaping the future of their respective industries. In the ever-evolving landscape of higher education, Northeastern University remains steadfast in its commitment to excellence, equity, and accessibility, preparing students to thrive in an increasingly complex and interconnected world. Through its unwavering dedication to academic rigor, experiential learning, and inclusive excellence, Northeastern continues to inspire and empower generations of students to lead lives of purpose, passion, and impact, leaving an indelible mark on the world around them.\\nAs Northeastern University continues to evolve and expand its reach, it remains dedicated to fostering a culture of innovation and collaboration both on campus and beyond. Through strategic partnerships with industry leaders, government agencies, and nonprofit organizations, Northeastern leverages its intellectual capital and research expertise to address some of the most pressing challenges facing society today. From pioneering advancements in healthcare and technology to driving sustainable development and social justice initiatives, Northeastern's interdisciplinary approach to problem-solving enables students and faculty to make meaningful contributions to the global community. Additionally, Northeastern's commitment to diversity, equity, and inclusion serves as a guiding principle in all aspects of university life, ensuring that every member of the community feels valued, respected, and empowered to succeed. With a forward-thinking vision and a steadfast dedication to excellence, Northeastern University continues to inspire and empower individuals to make a positive impact on the world, shaping a brighter future for generations to come.\\nAs Northeastern University embraces the challenges and opportunities of the 21st century, it remains at the forefront of innovation in education, research, and community engagement. With a focus on interdisciplinary collaboration and hands-on learning experiences, Northeastern equips students with the skills and knowledge needed to thrive in a rapidly changing world. Through its network of global campuses and partnerships, Northeastern provides students with unparalleled opportunities to engage with diverse cultures, tackle complex global issues, and develop a global mindset that transcends borders. Moreover, Northeastern's commitment to sustainability and environmental stewardship underscores its role as a leader in addressing the urgent challenges of climate change and environmental degradation. By integrating sustainability principles into its curriculum, operations, and campus initiatives, Northeastern is preparing students to become responsible stewards of the planet and agents of positive change in their communities.With a legacy of excellence and a vision for the future, Northeastern University continues to push the boundaries of knowledge, inspire innovation, and empower individuals to create a more just, and equitable world for all.\\nNortheastern University stands as a testament to the transformative power of education, innovation, and community engagement. Rooted in a tradition of academic excellence and fueled by a spirit of relentless curiosity, Northeastern empowers students to become lifelong learners, critical thinkers, and leaders in their respective fields. Through its cutting-edge research initiatives and collaborative partnerships, Northeastern tackles some of society's most pressing challenges, from healthcare disparities and urban revitalization to technological innovation and global security. Moreover, Northeastern's commitment to diversity, equity, and inclusion ensures that all members of its community have the opportunity to thrive and contribute their unique perspectives to the pursuit of knowledge. As Northeastern continues to push the boundaries of what is possible in higher education, it remains steadfast in its mission to inspire innovation and drive positive change in the world.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beaa0b29-a012-4f91-9d74-6c4f2aea2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = USER_CHAT_TEMPLATE.format(prompt=user_input + '<start_of_turn>model\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddfc9be-1f7d-4519-9e3a-834c35e9babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = tokenizer.encode(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6339822-35f3-484c-bef7-13b2aa3020c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed217100-8e13-409b-8ed4-c0b0f14b64e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Partition in all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "199d72ad-9352-43d5-a394-9e1239cf6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaLayerModel(nn.Module):\n",
    "    def __init__(self, config: gemma_config.GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = GemmaDecoderLayer(config)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, freqs_cis: torch.Tensor, kv_write_indices: torch.Tensor, kv_cache: Tuple[torch.Tensor, torch.Tensor], mask: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.layer(hidden_states=hidden_states, freqs_cis=freqs_cis, kv_write_indices=kv_write_indices, kv_cache=kv_cache, mask=mask)\n",
    "        return hidden_states\n",
    "    \n",
    "class GemmaLastLayerModel(nn.Module):\n",
    "    def __init__(self, config: gemma_config.GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = GemmaDecoderLayer(config)\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, freqs_cis: torch.Tensor, kv_write_indices: torch.Tensor, kv_cache: Tuple[torch.Tensor, torch.Tensor], mask: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.layer(hidden_states=hidden_states, freqs_cis=freqs_cis, kv_write_indices=kv_write_indices, kv_cache=kv_cache, mask=mask)\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f75811b-e22c-40f1-bec6-976e4d646753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, filepath):\n",
    "    \"\"\"Loads a model's state dictionary from a specified filepath.\"\"\"\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    \n",
    "def load_weights(model, model_path: str):\n",
    "    \"\"\"Original function used for weight loading in GemmaForCausalLM\"\"\"\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            model_path, mmap=True, weights_only=True,\n",
    "        )['model_state_dict'],\n",
    "        strict=False,\n",
    "    )\n",
    "    \n",
    "def load_adjusted_weights(model, adjusted_state_dict):\n",
    "    \"\"\"Loads the adjusted weights into the model, with strict=False to allow for incomplete state dicts.\"\"\"\n",
    "    model.load_state_dict(adjusted_state_dict, strict=False)\n",
    "    \n",
    "def save_model(model, filepath):\n",
    "    \"\"\"Saves a model's state dictionary to a specified filepath.\"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "def adjust_keys_for_ith_layer(state_dict, config, i):\n",
    "    \"\"\"Adjusts the state_dict's keys for the first layer model.\"\"\"\n",
    "    new_state_dict = {}\n",
    "    add_last_norm = config.num_hidden_layers == (i + 1)\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith(f'model.layers.{i}.'):\n",
    "            new_key = key.replace(f'model.layers.{i}.', f'layer.')\n",
    "            new_state_dict[new_key] = value\n",
    "        elif add_last_norm and key.startswith('model.norm.weight'): \n",
    "            new_state_dict['norm.weight'] = value\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "509a70a6-45dd-4928-9741-fecc0c6473f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "# Initialize and load weights for both models\n",
    "layer_model = GemmaLayerModel(config)\n",
    "last_layer_model = GemmaLastLayerModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d9d2abb-d386-4102-995a-b20de9f45a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# Load original model to be splitted\n",
    "model = GemmaForCausalLM(model_config)\n",
    "model.load_weights(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b134d5ee-45ef-41d3-aead-c2397c65d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_state_dict is the state dict loaded from the complete GemmaForCausalLM model\n",
    "full_model_state_dict = model.state_dict()\n",
    "\n",
    "for layer in range(config.num_hidden_layers - 1): \n",
    "    # Adjust and load weights for the first layer model\n",
    "    layer_state_dict = adjust_keys_for_ith_layer(full_model_state_dict, config, layer)\n",
    "    load_adjusted_weights(layer_model, layer_state_dict)\n",
    "    # Save the ith layer model\n",
    "    save_model(layer_model, f'./weights/{VARIANT}/layer_model_{layer}.pth')\n",
    "\n",
    "# Adjust and load weights for the remaining layers model\n",
    "last_layer_state_dict = adjust_keys_for_ith_layer(full_model_state_dict, config, config.num_hidden_layers - 1)\n",
    "load_adjusted_weights(last_layer_model, last_layer_state_dict)\n",
    "# Save the last layer model\n",
    "save_model(last_layer_model, f'./weights/{VARIANT}/layer_model_{config.num_hidden_layers - 1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f0ba815-b698-419f-92f3-6cbdc195fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is the loaded GemmaForCausalLM instance\n",
    "# Save Embedding weights\n",
    "torch.save(model.embedder.state_dict(), f'./weights/{VARIANT}/embedding_weights.pth')\n",
    "\n",
    "# Sampler does not have any trainable parameters or buffers to save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead255a-94f0-4df2-8b2c-4809eff74db3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference all layers splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54cd4db4-772d-4b06-84be-ced842607336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.2.5)\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the tokenizer is present\n",
    "weights_dir = kagglehub.model_download(f'google/gemma/pyTorch/{VARIANT}')\n",
    "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
    "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea0fc376-2b81-4d5c-9eeb-f642ad9798ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration\n",
    "MACHINE_TYPE = 'cuda'\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "config.tokenizer = tokenizer_path\n",
    "config.quant = 'quant' in VARIANT\n",
    "torch.set_default_dtype(config.get_dtype())\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "# Restart from saved models\n",
    "# Initialize and load splits\n",
    "model = [GemmaLayerModel(config) for layer in range(config.num_hidden_layers - 1)]\n",
    "model.append(GemmaLastLayerModel(config))\n",
    "for layer in range(config.num_hidden_layers):\n",
    "    load_model(model[layer], f'./weights/{VARIANT}/layer_model_{layer}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4abecae-9da1-4ab0-98d3-3d06e64a230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# Hidden size needs to be divisible by the number of heads\n",
    "assert config.hidden_size % config.num_attention_heads == 0\n",
    "\n",
    "# Prepare hidden_states, freqs_cis, kv_write_indices, kv_cache, and mask for inference\n",
    "max_seq_len = config.max_position_embeddings\n",
    "head_dim = config.head_dim\n",
    "vocab_size = config.vocab_size\n",
    "\n",
    "tokenizer = Tokenizer(config.tokenizer)\n",
    "# Initialize embedder\n",
    "embedder = Embedding(vocab_size, config.hidden_size, config.quant).to(device)\n",
    "embedding_weights = torch.load(f'./weights/{VARIANT}/embedding_weights.pth', map_location=device)\n",
    "embedder.load_state_dict(embedding_weights)\n",
    "model = [layer.to(device).eval() for layer in model]\n",
    "# Initialize sampler\n",
    "sampler = Sampler(vocab_size).to(device)\n",
    "# Pre-compute rotary embedding table.\n",
    "rope_theta = getattr(config, 'rope_theta', 10000)\n",
    "prec_freqs_cis = precompute_freqs_cis(head_dim,\n",
    "                                 max_seq_len * 2,\n",
    "                                 theta=rope_theta).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585e691-e52b-485c-a250-c2a1bb09a750",
   "metadata": {},
   "source": [
    "Forward and generate functions are still the same from [split model inference](##Split-model-inference) section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00592acf-f81b-4c56-a7cc-474e048d2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model[13] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a36e8258-cd4a-4701-ab71-7ab20096cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat prompt:\n",
      " <start_of_turn>user\n",
      "What is the best city in Europe?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Barcelona.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "What can I do in Barcelona?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"## Things to do in Barcelona:\\n\\n**Culture & History:**\\n\\n* Explore the Gothic Quarter with its narrow cobblestone streets and historic architecture.\\n* Visit La Sagrada Familia, Gaudi's unfinished masterpiece.\\n* See the Casa Batlló and Casa Milà, two more of Gaudi's iconic buildings.\\n* Explore Park Güell, a hilltop park with stunning views of the city.\\n* Visit the Picasso Museum and learn about the famous artist's early works\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate with one request in chat mode\n",
    "\n",
    "# Chat templates\n",
    "USER_CHAT_TEMPLATE = '<start_of_turn>user\\n{prompt}<end_of_turn>\\n'\n",
    "MODEL_CHAT_TEMPLATE = '<start_of_turn>model\\n{prompt}<end_of_turn>\\n'\n",
    "\n",
    "# Sample formatted prompt\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(\n",
    "        prompt='What is the best city in Europe?'\n",
    "    )\n",
    "    + MODEL_CHAT_TEMPLATE.format(prompt='Barcelona.')\n",
    "    + USER_CHAT_TEMPLATE.format(prompt='What can I do in Barcelona?')\n",
    "    + '<start_of_turn>model\\n'\n",
    ")\n",
    "print('Chat prompt:\\n', prompt)\n",
    "\n",
    "generate(\n",
    "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
    "    device=device,\n",
    "    output_len=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f305885-402e-44d9-ab1e-3e34d85d8330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
